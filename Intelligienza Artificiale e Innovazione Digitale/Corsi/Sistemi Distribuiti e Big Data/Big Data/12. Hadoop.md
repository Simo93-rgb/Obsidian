
# Architettura

![[12. Hadoop - Architecture.png]]

The Hadoop cluster: logical topology
* A *cluster* is a group of computers working together
* Provides data storage, data processing, and resource management
* A *node* is an individual computer in the cluster
* Master nodes manage distribution of work and data to worker nodes
* A *daemon* (or *service*) is a program running on a node
* Each Hadoop daemon performs a specific function in the cluster

Ci sono 3 concetti principali:
1. Storage: dove salviamo i dati
2. Gestore risorse: dove processare i dati
3. Processamento: come processare i dati
![[12. Hadoop - RM Processing Storage.png|400]]

Insieme fanno il sistema distribuito. 

## Componenti principali
![[12. Hadoop - Layers.png]]
* *HDFS*: file system distribuito pensato per una banda elevata ed è lo storage layer.
* *YARN*: gestore delle risorse ossia uno scheduler dei lavori
* *MapReduce*: sistema basato su YARN per il processamento di batch distribuiti. 

# HDFS

HDFS sta per Hadopp Distribuited File System e deriva da Google File System. 
##### Key assumptions:
- Memorizza file di grandi dimensioni (dimensione del file GB). 
	- Ad esempio, preferiscono milioni di file MB 100 piuttosto che miliardi di file 10 KB. 
- I fascicoli sono generalmente letti in sequenza o allegati. 
	- Un set di dati è tipicamente generato o copiato da una fonte e quindi una grande proporzione (se non tutto) di esso viene letto più volte (principio "scrivi una volta, leggi molte volte"). 
	- Piccole scritture casuali sono praticamente inesistenti, mentre possono esistere grandi operazioni di aggiunta. 
- Funziona su cluster di molti server commodity. 
	- Dal momento che i server di base sono di solito inaffidabili, si aspettano un alto tasso di fallimento Throughput è più importante di latenza. 
	- Leggere l'insieme completo dei dati nel modo più rapido possibile è più importante della latenza per ottenere un singolo record casuale dal set di dati. 
- Non è necessario essere completamente conformi allo standard POSIX (ad esempio, nessun collegamento simbolico, ...)

##### Key design:
- blocchi di taglia fissa, di base 128B.
- I files sono *write-once* e poi si fanno solo append e truncate
- si replicano i blocchi per la *fault-tollerance*, minimo 3 e su nodi differenti.

##### HDFS id bad at
funziona male sulle scritture casuali, nel gestire piccoli file e nel dare bassa latenza.

#### NameNode
Operazioni di namespace del file system, salva metadati e performa I/O. È molto critica come componente. Un NameNode secondario può essere usato ma solo come aiuto non come backup. 
##### NameNode Secondario
- Il NameNode memorizza i metadati del filesystem sul disco (per la memorizzazione della persistenza) in due file: 
	- FSimage: memorizza un'istantanea dello spazio dei nomi del filesystem Anche memorizzato in memoria per motivi di efficienza 
	- EditLog: memorizza le modifiche apportate ai metadati del file system wrt l'istantanea in FSImage 
- Ogni volta che viene riavviato il NameNode, le modifiche in EditLog vengono applicate a FSImage per ottenere l'ultima istantanea dello spazio dei nomi del file system (checkpoint)
- Ma i riavvii NameNode sono rari nei cluster di produzione 
	- EditLog diventa molto grande 
	- Quando il riavvio NameNode, ci vuole molto tempo per applicare a FSImage tutte le transazioni registrate nel (molto grande) EditLog 
	- Durante questo lungo periodo, il filesystem è offline, che non è desiderabile
È sempre su un computer separato e ha bisogno delle stesse risorse del NmeNode principale.

#### DataNode
- Serve richieste di lettura e scrittura dai client HDFS 
- Esegue la creazione di blocchi, la cancellazione e la replica su istruzione del NameNode
- Invia periodicamente messaggi heartbeat al NameNode 
	- Ogni 3 sec., by default 
	- Ogni messaggio contiene l'identità del datanode e l'elenco dei blocchi che sta memorizzando 
	- Questa informazione è usata da NameNode per confermare che 
		- Il datanode è vivo 
		- I blocchi che sono attualmente memorizzati nel datanode

#### Replicazione
La policy di default è **Rack-aware replica placement**:
- La prima replica viene posizionata su un datanode nello stesso rack del client
- la seconda in un DataNode differente dal primo
- La terza replica nello stesso DataNode della seconda
- Tutte le altre sono messe casualmente.

![[12. Hadoop - HDFS Example.png]]
##### Write
![[12. Hadoop - HDFS Example Write.png]]

##### Read
![[12. Hadoop - HDFS Example Paste.png]]

# YARN
YARN sta per **Yet Another Resource Negotiator** ed è il manager delle risorse e job scheduler. Da Hadoop 2 supporta anche applicazioni non strettamente MapReduce.
Migliora la clusterizzazione, la scalabilità e la legacy visto che le applicazioni Hadoop 1 girano perfettamente.
I componenti principali sono:
1. Container
2. Resource Manager
3. Node Manager
4. Application Master
Definiamo un *job* una applicazione YARN in esecuzione, un *task* una unità di un job che gira in un container, *worer node* è il nodo che fa girare i container. 

##### Container
- Allocazione logica delle risorse (RAM, CPU, disco, ...) di un singolo nodo per eseguire un processo specifico dell'applicazione
- È l'unità di base di lavoro in FILATO
- Ci possono essere molti contenitori associati alla stessa applicazione
- Può essere eseguito come un processo OS tradizionale, un cgroups Linux, un contenitore Windows Secure o un contenitore Docker

![[12. Hadoop - YARN.png|400]]

##### Resource Manager
Crea e monitora processi ApplicationMaster e c'è solo una istanza attiva in un cluster. 
- Tiene traccia delle risorse nel cluster e nelle applicazioni di pianificazione (ad esempio, i lavori MapReduce)
- Allocare le risorse del cluster disponibili alle attività di applicazioni concorrenti
	- Le applicazioni negoziano per le risorse con esso
	- Istruisce NodeManagers per allocare risorse
- Ha uno scheduler pluggable per supportare diversi algoritmi (ad esempio, scheduler FIFO, capacity scheduler, fair scheduler, ...)
- Crea e monitora i processi ApplicationMaster
- Esiste un'istanza "attiva" unica in un cluster
	- Di solito viene eseguito su un nodo master dedicato
- Prima di Hadoop 2.4, ResourceManager è il singolo punto di guasto in un cluster YARN
- Da Hadoop 2.4: ResourceManager Alta disponibilità
	- C'è un Active ResourceManager e uno o più Standby ResourceManagers, che prendono il sopravvento se dovesse accadere qualcosa all'Active ResourceManager

##### Node Manager
- Fornisce le risorse del nodo alle attività dell'applicazione sotto forma di container
- Monitora l'utilizzo delle risorse dei container in esecuzione e lo riporta a ResourceManager
- Un NodeManager per nodo lavoratore

##### Application Manager
- Gestisce l'esecuzione di un'applicazione YARN
	- Responsabile del ciclo di vita dell'applicazione
	- Quadro/specifico per l'applicazione
- Chiede ai contenitori di eseguire attività applicative
	- Negozia le risorse dal ResourceManager e interagisce con il NodeManager (s) per eseguire e monitorare i compiti
- Uno per ogni applicazione YARN in esecuzione
- Funziona in un contenitore

#### Funzionamento
![[12. Hadoop - YARN Example.png]]

##### Riassuntazzo
- Gli utenti devono essere in grado di 
	- inviare i lavori (applicazioni) per l'esecuzione sul cluster YARN 
	- Monitorare e gestire i lavori
- Hadoop include alcuni strumenti YARN
	- La linea di comando YARN Web UI
- È possibile accedere a YARN anche tramite un'API Java
	- Ad esempio, utilizzato dai vari motori di elaborazione dati distribuiti che si trovano sopra YARN (ad esempio, MapReduce, Spark, Flink, ...)

# Map Reduce
Ci sono due tipi primitivi per programmarci e sono le liste di elementi e le coppie chiave-valore. È possibile mischiare i concetti quindi avere chiavi con valore una lista piuttosto che una lista di coppie key-value. 

**Map**
È una funzione del tipo: $<k_1,v_1> \rightarrow list(<k_2,v_2>)$.
 
**Reduce**
È una funzione del tipo: $<k_2,list(v_2)> \rightarrow list(<k_3,v_3>)$.

Questo modello è essenzialmente un DAG con due rank di vertici ![[12. Hadoop - mapreduce dag.png]]
