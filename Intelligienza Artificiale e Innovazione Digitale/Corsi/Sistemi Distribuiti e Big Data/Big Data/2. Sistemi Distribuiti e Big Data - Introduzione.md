## L'era dei Big Data

- Nel 2013, il 90% di tutti i dati nel mondo è stato generato negli ultimi due anni.
- La International Data Corporation (IDC) nel 2014 ha previsto una crescita del volume globale dei dati digitali da 4,4 zettabyte (ZB) nel 2013 a 44 ZB entro il 2020. Questa previsione è stata rivista nel 2018: da 33 ZB nel 2018 a 175 ZB entro il 2025.
- A novembre 2022, la popolazione mondiale è di circa 8 miliardi di persone, con circa 5 miliardi di persone connesse a Internet, il che rappresenta circa il 63% della popolazione mondiale.

### Origine del flusso di dati

- Ricerca web, social media, app di messaggistica istantanea, e-commerce, esperimenti scientifici su larga scala, dispositivi Internet of Things (IoT).

### Statistiche

- La New York Stock Exchange genera circa 4-5 TB di dati al giorno.
- Facebook ospita oltre 240 miliardi di foto, con una crescita di 7 petabyte (PB) al mese.
- CISCO ha stimato che 50 miliardi di dispositivi IoT saranno connessi a Internet entro il 2020.

## Caratteristiche dei Big Data (3+1 V)

- **Volume**: la quantità di dati generati è enorme e in continua crescita.
- **Varieà**: i dati possono provenire da diverse fonti e variare molto in tipo (testo, immagini, audio) e in formato (dati strutturati, non strutturati o semi-strutturati).
- **Velocità**: i dati possono arrivare a velocità diverse e enormi dataset possono accumularsi in brevi periodi di tempo.
- **Veracità**: si riferisce alla qualità e affidabilità dei dati.

## Applicazioni dei Big Data

- Tracciamento epidemie in tempo reale, analisi dei clienti, manutenzione predittiva, pubblicità personalizzata, scoperte astronomiche, previsioni in tempo reale del mercato azionario, gestione del traffico urbano, analisi web.

## Processo di analisi dei Big Data

1. **Acquisizione dei dati**: raccogliere dati grezzi da molteplici e potenzialmente diverse fonti.
2. **Preparazione dei dati**: filtrare e trasformare i dati grezzi in un formato utilizzabile dai framework di elaborazione dei dati.
3. **Analisi dei dati**: confermare o scoprire nuove intuizioni e conoscenze.
4. **Accesso ai dati**: accedere, interpretare e utilizzare i risultati dell'analisi per estrarre informazioni di alto valore.

## Sfide nello storage, elaborazione e visualizzazione dei Big Data

- Enorme quantità di dati, crescita rapida dei dati, risorse di calcolo/storage limitate localmente.

## Architetture, modelli di computazione e algoritmi adatti al processing dei Big Data

- Supporto al processamento parallelo data-intensive, gestione efficiente delle risorse hardware/software, scalabilità rispetto al volume di dati.

## Scalabilità

- **Scalabilità verticale**: aumentare la capacità di calcolo/storage di un singolo computer.
- **Scalabilità orizzontale**: aggiungere più dischi per distribuire diversi chunk di dati e più nodi di calcolo per elaborare questi chunk in parallelo.

## Computazione su cluster

- L'elaborazione dei dati intensivi viene eseguita su un sistema di computer cluster, costituito da migliaia di nodi connessi da reti locali ad alta velocità.

## Architettura shared-nothing

- Un cluster di server commodity economici, ognuno dotato di storage dedicato, su cui copiare parte dei dati di input da analizzare.

## Dove si svolge l'analisi dei Big Data?

- On-premise, nel cloud, o nel continuum edge-to-cloud, ognuno con i propri pro e contro in termini di scalabilità, gestione dell'infrastruttura hardware/software e preoccupazioni