Problemi a miglioramento iterativo. Spesso ci manca proprio lo stato obbiettivo e non solo vogliamo trovarlo ma vogliamo trovare il migliore. 

# Algoritmi con miglioramenti iterativi
Qui si appiccica, per motivi storici, una funzione di utilit√†  e massimizziamo questa funzione ma non cambia nulla se avessimo bisogno di avere una funzione di costo e minimizzare di conseguenza. 
In molti problemi lo stato finale √® soluzione senza tenere conto del cammino che risulter√† quindi irrilevante. Quindi mantengo lo stato buono e ne cerco uno migliore.
#### Spazio degli stati
Insieme di configurazioni complete. Trova la soluzione **ottima** come nel caso del commesso viaggiatore; trova la soluzione che **soddisfa tutti i vincoli** come nel timetabling.

## Hill-climbing 
C'√® un detto: "*√® come scalare l'Everest in mezzo ad una fitta nebbia e soffrendo di amnesia*".
![[Iterative Improvement (Local) search - LocalMax_VS_GlobalMax.png|450]]
Questo algoritmo pu√≤ rimanere bloccato in un massimo locale (come da immagine). Una volta sul cucuzzolo della collina tutte le mosse peggiorano e quindi non scendo pi√π. 

<center style="font-style:italic">Pseudo codice del'algoritmo Hill-Climbing</center>

```lua
function Ricerca-Salita-Pi√π-Ripida(problema) returns uno stato soluzione
  inputs: problema, un problema
  
  local variables: corrente, un nodo
                   prox, un nodo
  
  corrente ‚Üê Costruisci-Nodo(Stato-Iniziale[problema])
  loop do
    prox ‚Üê successore di corrente con valore max
    if Valore[prox] < Valore[corrente]
      then return corrente
    corrente ‚Üê prox
  end

```

Siccome rimango bloccato, potrei pensare di fare delle ripartenze dell'algoritmo in stati iniziali differenti per evitare il fenomeno di *get stuck* nel massimo locale. Questo concetto si chiama **random start**.
### [[https://en.wikipedia.org/wiki/Hill_climbing#Ridges_and_alleys|Ridges and alleys]]
- **Ridges (Creste)**: Questi sono i punti elevati della superficie di ricerca che rappresentano soluzioni di alta qualit√† secondo la funzione obiettivo. In termini di ottimizzazione, una cresta corrisponderebbe a un massimo locale, ovvero un punto in cui tutte le direzioni immediate portano a un decremento del valore della funzione obiettivo. Queste sono le zone che un algoritmo di ricerca a salita (hill-climbing) tenta di trovare e seguire.
    
- **Alleys (Valli)**: Al contrario, le valli sono punti bassi che corrispondono a minimi locali. In una ricerca di ottimizzazione, una valle √® un punto dove un piccolo movimento in qualsiasi direzione porter√† a un incremento del valore della funzione obiettivo. Queste sono le aree che un algoritmo di ricerca a salita cerca di evitare o uscire, poich√© suggeriscono che ci sono soluzioni migliori altrove.
![[Iterative Improvement (Local) search - RidgeProblem.png|450]]
La ricerca a salita pi√π ripida, o algoritmo del gradiente, cerca di spostarsi sempre "in salita" sulla superficie di ricerca, seguendo il gradiente positivo della funzione obiettivo. Tuttavia, questa strategia pu√≤ avere difficolt√† quando incontra creste, valli, o altri tipi di irregolarit√† (come piani inclinati, sella, ecc.) poich√© l'algoritmo pu√≤ rimanere bloccato in massimi locali senza trovare il massimo globale se la superficie di ricerca √® complessa con molte creste e valli.
### Esempio: MMMS
**Minimal Makespan Machine Scheduling** ossia l'ordinamento l'ordinamento di job su macchine minimizzando il tempo di esecuzione:
+ Abbiamo $n$ macchine $\{M_1, ..., M_n\}$ e $m$ lavori $\{1, ..., m\}$
+ $N(i)$ set di lavori assegnati a $M_i$
+ Il tempo del lavoro $k$ √® $d_k$
+ Tempo $M_i$ √®: $T(i) = \sum_{j\in N(i)}{d_j}$
+ Il tempo totale $T_{tot} = max(T_i)$
+ Obbiettivo: Minimizzare $T$ assegnando opportunamente i lavori alle macchine

#### Applico Hill-Climbing a MMMS
Parto da uno stato a caso e i miei operatori sono:
+ spostamento
+ scambio
La valutazione dello stato √® sempre $T$ e mi fermo se tutti gli stati successori sono peggiori di quello corrente. Nell'esempio sotto i due punti separano i lavori della macchina $M_1$ a sinistra da quelli della macchina $M_2$ a destra, i lavori allo stato iniziale sono messi a caso. Considerare un operatore ```spostamento``` che sposta un lavoro da una macchina ad un'altra. 

```mermaid 
graph TD 

id1((5 4 4 : 3 2)) --> id2((5 4 4 : 3 2))
id1((5 4 4 : 3 2)) --> id3((4 4 : 5 3 2))
id1((5 4 4 : 3 2)) --> id4((5 4 : 4 3 2))
```
Lo stato di partenza √® appunto un set random dei lavori e provando a sommare i tempi otterrei $T_1=13$ e $T_2=5$ con un evidente sbilanciamento; con il secondo nodo otterrei rispettivamente $8$ e $10$ . Col terzo nodo ottengo $T_1=9$ e $T_2=9$, che non solo √® pi√π bilanciato ma √® soluzione ottima perch√© ho minimizzato $T$. L'algoritmo si ferma quando nessuna delle modifiche possibili dello stato fa ottenere uno stato migliore del precedente.  
Precedentemente √® stato menzionato l'approccio **random start** alla fine del paragrafo [[1.4 Iterative Improvement (Local) search#Hill-climbing|Hill-Climbing]] ma esistono altri due approcci al problema dei massimi locali:
+ Stochastic Hill-Climbing $\rightarrow$ scelgo statisticamente il successore purch√© migliore del precedente
+ Simulated Annealing $\rightarrow$ porta pazienza lo vediamo fra pochissimo qui sotto üòÅ
**RICHIAMO: Normalizzazione a 1**
L'operazione di normalizzazione a 1 ci permette di poter lavorare con una distribuzione di prob. che deve avere valori nell'intervallo $[0,1]$. Si tratta di dividere il valore per il totale dei valori.

### Simulated Annealing - SA (Tempra Simulata)
Accettiamo l'idea di poter fare una **bad move (mossa cattiva)** ossia una mossa che mi porta da uno stato corrente migliore ad uno futuro peggiore. Questa cosa va fatta con due criteri:
1. La prob. della mossa deve essere inversamente proporzionale a quanto √® cattiva la mossa
2. Ad inizio algoritmo permetto facilmente mosse cattive ma la frequenza di queste deve diminuire man mano che si avanza nella ricerca.
	1. Se l'algoritmo ha $p = 0$ di eseguire una bad move allora √® un hill-climbing puro.
#### Probabilit√† di una mossa
Sia $\Delta \mathbb{E} = Value(S') - Value(S)$:
+ Se la mossa √® cattiva allora $\Delta \mathbb{E} < 0$
+ $p = e^{\Delta\mathbb{E}}$
	+ Questa $p$ mi consente di avere una una probabilit√† tanto pi√π piccola quanto pi√π grande √® il valore di $|\Delta \mathbb{E}|$ 
![[Iterative Improvement (Local) search - probabilit√† simulated annealing.png|350]]
Se $\Delta \mathbb{E} > 0$ faccio la mossa che mi porta in $S'$, altrimenti passo da $S$ a $S'$ con una probabilit√† $p$. In pratica genero un numero casuale $x$ e se questo valore √® minore di $p$ allora eseguo la mossa. Per fare in modo che la frequenza delle mosse cattive diminuisca con l'avanzamento dell'algoritmo introduco un parametro $T$ che rappresenta la temperatura ed esattamente come la tempra la faccio partire da valori elevati che poi decrescono. Si chiama anche *scheduling di raffreddamento* La nuova funzione di prob √®: $$p = e^{{\Delta \mathbb{E} \over T}}$$Quando $T = 0$ allora SA diventa hill-climbing; questa cosa √® imposta siccome non si pu√≤ dividere per zero!
```lua
function Simulated-Annealing(problema, ordine) returns uno stato soluzione
  inputs: problema, un problema
           ordine, relazione tra tempo e "temperatura"
  local variables: corrente, un nodo
                   prox, un nodo
                   T, "temperatura"
  
  corrente ‚Üê Costruisci-Nodo(Stato-Iniziale[problema])
  for t ‚Üê 1 to ‚àû do
    T ‚Üê ordine[t]
    if T = 0 then return corrente
    prox ‚Üê successore di corrente selezionato a caso
    ŒîE ‚Üê Valore[prox] - Valore[corrente]
    if ŒîE > 0 then corrente ‚Üê prox
    else corrente ‚Üê prox con probabilit√† e^(ŒîE / T)
  end

```

#### Propriet√† Simulated Annealing (non importante)
Sostanzialmente √® una tempra simulata, e fin l√¨... A temperatura fissata la probabilit√† segue la *distribuzione di Boltzman*: $$p(x)=\alpha e^{\frac{E(x)}{kT}}$$Se la decrescita di $T$ √® abbastanza lenta allora si raggiunge sempre lo stato ottimo. 
## Local Beam Search 
### Intro (solo questo √® importante)
Il Local Beam Search √® una variante degli algoritmi di ricerca locale che opera con pi√π di un "candidato" di stato contemporaneamente. A differenza della ricerca a salita pi√π ripida che tiene traccia di un solo stato corrente e si muove sempre verso il miglior vicino, il Local Beam Search tiene traccia di $k$ stati contemporaneamente, dove $k$ √® un parametro dell'algoritmo detto, appunto, *beam*. 

### Funzionamento
Ecco come funziona il Local Beam Search:

1. **Inizio**: Genera inizialmente $k$ stati casuali.
    
2. **Iterazioni**: Ad ogni iterazione, l'algoritmo esplora tutti i vicini di tutti $k$ stati correnti.
    
3. **Selezione**: Dopo aver esplorato i vicini, seleziona i $k$ migliori stati (secondo una funzione obiettivo o di valutazione) tra tutti quelli esaminati, inclusi i $k$ stati correnti e i loro vicini.
    
4. **Criterio di terminazione**: Questo processo continua fino a quando non si verifica una delle seguenti condizioni:
    
    - Non si trovano stati migliori rispetto agli attuali $k$ stati, indicando che l'algoritmo √® bloccato in un massimo locale.
    - Si raggiunge un numero massimo di iterazioni.
    - Uno degli stati raggiunge un valore di valutazione considerato sufficientemente buono o si trova una soluzione accettabile.

La differenza principale tra la ricerca a salita pi√π ripida e il Local Beam Search √® che quest'ultimo √® meno propenso a rimanere bloccato in un massimo locale, dato che tiene traccia di pi√π stati e pu√≤ quindi esplorare pi√π ampiamente lo spazio di ricerca. Tuttavia, anche il Local Beam Search non √® esente da problemi: pu√≤ convergere su un insieme di stati simili che rappresentano lo stesso massimo locale, una situazione nota come "raggi convergenti".

Inoltre, una variante del Local Beam Search √® lo "Stochastic Beam Search", che introduce elementi di casualit√† nella selezione degli stati da esplorare. Invece di scegliere sempre i $k$ migliori vicini, lo Stochastic Beam Search seleziona i vicini in base a una distribuzione di probabilit√† relativa alla loro qualit√†. Questo aiuta a diversificare l'insieme di stati e riduce il rischio di convergenza prematura.

# Algoritmi Genetici
La metafora qui √® l'evoluzione Darwiniana ossia quella genetica. Segue il principio dello stochastic beam search solo che gli stati successori sono generati da coppie di stati. Gli stati sono stringhe o bit chiamati **geni** i cui valori sono detti **alleli**. Quindi uno stato √® un insieme di geni ed √® detto **individuo** e un insieme di individui √® una **popolazione**. 
Lo scopo √® far evolvere una popolazione iniziale tramite operazioni in modo da massimizzare la funzione di utilit√† detta **fitness function**. Le operazioni sono:
+ **crossover**: incrocio i geni definendo un punto di taglio casuale.
+ **selezione**: tipo la selezione naturale dove vince il pi√π forte, i migliori in termini di fitness function
+ **mutazione**: in modo completamente casuale c'√® una variazione di valore un po' come avviene con le mutazioni genetiche.
Lo stato soluzione del problema √® quello che prende, tramite fitness function, l'individuo migliore nella nuova popolazione generata.
Nonostante non siano ottimali sono comunque molto utili in molti ambiti.
## Esempio 
![[Iterative Improvement (Local) search - Esempio_algoritmo_genetico.png]]
A sinistra (punto a) abbiamo la popolazione allo stato iniziale. La fitness function (punto b: numero intero a SX) viene trasformata in una prob. di selezione (punto b: le percentuali a DX) grazie ad una semplice normalizzazione a 1. Si decide di selezionare per 4 volte (punto c) gli individui della popolazione. L'operazione di selezione (punto c) scarta il quarto individuo e incrocia gli individui come indicato dalle frecce. Le coppie selezionate e scambiate di posizione verranno fatte evolvere con l'operazione di crossover  (punto d) definendo casualmente un punto di taglio, come rappresentato durante l'operazione di selezione (punto c). L'ultima fase √® quella di mutazione (punto e) che prende la nuova generazione appena creata e in modo completamente randomico cambia il valore di un gene (a volte pi√π geni, dipende da come si fa l'algoritmo in base alle esigenze). Attenzione, la prob. che un gene muti √® bassissima, qualcosa tipo $10^{-6}$.
I numeri dell'esempio sopra rappresentano le regine del problema delle 8 regine.
![[Iterative Improvement (Local) search - 8_Regine_algoGenetico.png]]
