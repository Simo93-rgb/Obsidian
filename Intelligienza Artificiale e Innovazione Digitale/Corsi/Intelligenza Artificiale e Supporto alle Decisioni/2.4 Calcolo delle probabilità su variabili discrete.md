# Elementi di Probabilità
Qui diventa inutile ripetere le stesse cose viste a statistica, la nomenclatura è pure quasi la stessa. [[1. Richiami di Probabilità|Qui]] la nota.
Alcune precisazioni su come verranno indicate le cose in questa sezione:
- La proposizione $a = \text{True}$ corrisponde all'evento $A = \text{insieme di eventi elementari per cui } a= \text{ True}$
	- Ad esempio se $D={1,4,5}$ allora $d$ è proposizione e $D$ è evento
	- Se si lavora su insiemi si usa la notazione insiemistica mentre se si lavora su proposizioni si usa la notazione logica.
- Vedere gli [assiomi di Kolmogorov](https://it.wikipedia.org/wiki/Andrej_Nikolaevi%C4%8D_Kolmogorov) Questi sono riportati anche [[1. Richiami di Probabilità|nella nota]] di statistica
- Vedere la [[2.3 Formula di Bayes e Teorema Probabilità Totali#Bayes Probabilità condizionata|probabilità condizionata]] 
	- In IA il concetto di eventi mutualmente esclusivi ed esaustivi servono per descrivere quelle situazioni dove ho un elenco finito di ipotesi e solo quelle sono le mie possibilità. 
- Vedere anche il [[2.3 Formula di Bayes e Teorema Probabilità Totali#Teorema delle Probabilità Totali|Teorema Probabilità Totali]]
	- **Attenzione** che con *indipendenza* indichiamo quando il verificarsi di un evento non ha alcuna relazione col verificarsi di un altro, mentre con *mutua esclusione* gli eventi sono decisamente dipendenti fra loro perché il verificarsi di un evento implica necessariamente il non verificarsi dell'altro. Quindi mutua esclusione implica dipendenza.

Siccome in IA spesso si ha la necessità di rappresentare due eventi distinti vien da sé che servono due variabili casuali - qui indicate da rv da random variable - che dovranno essere necessariamente combinate con una qualche diavoleria statistica. 
## Probabilità congiunte
Abbiamo due vc $A$ e $B$ con eventi atomici $\{a_1, a_2\}$ e $\{b_1, b_2\}$ rispettivamente, andrò ad indicare con $\mathbb{P}(A,B)$ la probabilità congiunta di $A$ e $B$ che sarà fatta dal prodotto cartesiano degli elementi elementari delle due vc. $$\mathbb{P}(A,B) = \left\{\begin{array}{ll}\mathbb{P}([a_1,b_1]) \\\mathbb{P}([a_1,b_2]) \\ \mathbb{P}([a_2,b_1]) \\ \mathbb{P}([a_2,b_2]) \\ \end{array}\right\}$$
Se ho la probabilità congiunta posso ottenere le probabilità marginali che nel caso qui sopra sono $\mathbb{P}(A)$ e $\mathbb{P}(B)$. L'operazione di passaggio dalla congiunta alla marginale è della *operazione di marginalizzazione*. 

### Digressione sulla statistica
Formalmente la statistica è definita dagli assiomi di Kolmogorov ma questo non impedisce di dare diverse interpretazioni pur rispettando questi ultimi. Ci sono due scuole di pensiero che meritano di essere menzionate:
- **Approccio Frequentista** (Von Mises)
	- statistica classica
	- Un evento può essere ripetuto indefinite volte 
	- $\lim_{n_=\infty}{S^n(E) \over n} = \mathbb{P}(E)$
		- Dove $S^n(E)$ è il numero di volte che l'esperimento E ha fato un risultato 
		- $n$ è il numero di esperimenti totali. 
- **Approccio Soggettivista** (de Finetti)
	- statistica Bayesiana
	- La probabilità è un concetto soggettivo ma assegnato in modo coerente
	- C'è il concetto di scommessa equa, ossia se ho un evento unico e irripetibile la $\mathbb{P}(E)$ è data dalla quantità di denaro che si è disposta a scambiare ricevendo 1 se vinco e zero se perdo. 
		- Coerenza la intendo che se potessi ripetere più volte lo scambio in media non vinco e non perdo.

In IA è moto più popolare l'approccio soggettivista perché meglio si sposa con la natura dei problemi che in IA ci si trova a modellare. 
La [[2.3 Formula di Bayes e Teorema Probabilità Totali#Bayes Probabilità condizionata|Formula di Bayes]] diviene importantissima soprattutto bisogna imparare a masticare i concetti di *probabilità a priori* e *probabilità a posteriori*. Secondo i frequentisti non bisognerebbe usare la prob. a posteriori ma per noi vige il **STI CAZZI**. 

## Naïve Bayes
Siccome c'è difficoltà a generalizzare il teorema di Bayes quando ho molte evidenze da considerare posso pensare di fare una assunzione per semplificare il sistema. Ad esempio posso dire che due sintomi non dipendono l'uno dall'altro se conosco la malattia di cui soffre il paziente. Più formalmente, *le evidenze sono indipendenti fra loro data l'ipotesi*.
$$\begin{align}&\quad \mathbb{P}(h_i|e_1...e_m)={\mathbb{P}(h_i)\mathbb{P}(e_1...e_m|h_i)\over \sum_{k=1}^n\mathbb{P}(h_k)\mathbb{P}(e_1...e_m|h_k)}\\ \\ &\quad \text{se le evidenze sono indipendenti data l'ipotesi, allora}\\\ \ &\quad \mathbb{P}(h_i|e_1...e_m)={\mathbb{P}(h_i)\prod_{j=1}^m\mathbb{P}(e_j|h_i)\over \sum_{k=1}^n\mathbb{P}(h_k)\prod_{j=1}^m\mathbb{P}(e_j|h_k)}\end{align}$$