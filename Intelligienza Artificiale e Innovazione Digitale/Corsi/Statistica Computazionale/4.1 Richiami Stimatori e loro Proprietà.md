
# Statistica: definizione
Dicendo che *una statistica è funzione del campione* si sta sottolineando un concetto fondamentale della statistica inferenziale. In questo contesto, una "statistica" non è solo un valore numerico che riassume i dati, ma specificamente un valore calcolato dai dati di un campione, che serve per fare inferenze sulla popolazione da cui il campione è stato estratto. 
Una statistica è definita come qualsiasi funzione dei dati del campione che non dipende da parametri sconosciuti della popolazione. Questo significa che il calcolo di una statistica utilizza solo i valori dei dati nel campione e non incorpora alcuna informazione esterna o presupposti non verificabili sui dati della popolazione globale.
$$\mathcal{T}=T(X_1,...,X_n)$$
Ad esempio la media campionaria e la varianza campionaria sono delle statistiche:
- Media campionaria ($\bar x$): è la somma di tutti i valori nel campione divisa per il numero di osservazioni nel campione. È una statistica perché è calcolata direttamente dai dati del campione senza fare riferimento a parametri della popolazione.
- Varianza campionaria ($S^2$): è una misura della dispersione dei dati del campione intorno alla loro media campionaria. Anche questa è una statistica, poiché si basa solo sui dati del campione.
- Proporzione campionaria ($p$): in studi su caratteristiche categoriche, è la frazione di osservazioni nel campione che appartengono a una certa categoria. Anche questa è una statistica, derivata direttamente dal campione.

## Empirical CDF
Serve per simulare la cdf teorica ed essendo costruita direttamente dai dati è anch'essa una statistica. <u>È una funzione a gradini</u> che aumenta di $1\over n$ ad ogni valore osservato, dove $n$ è la dimensione del campione $X_1,..,X_n$ iid come $X$. Formalmente così definita: $$F_n(x)=\sum_{i=1}^n I_X(x_i\leq x)$$dove $I_X(\bullet)$ è la *funzione indicatrice* $X$. Il prof la indica con $\mathbb{1}_S$ ma per questioni grafiche ho preferito prendere la nomenclatura standard e usare $I_X$ con $X$ l'insieme a cui si applica. La funzione indicatrice è definita così $$
I_{\Omega}(\omega)=
\begin{cases}
1 \quad \text{se}\quad \omega\in X \\
0 \quad \text{se}\quad \omega\notin X
\end{cases}
$$Secondo il [[2.2 TLC e LGN#Teorema del Limite Centrale|TLC]] la e.cdf tende alla cdf teorica. Dalla e.cdf posso calcolare i quantili empirici.

# Stimatori Corretti
Uno stimatore $\hat \Theta$ è *corretto* o *non distorto* (unbiased) per un parametro $\Theta$ se vale l'identità:$$\mathbb{E}[\hat \Theta]=\Theta$$La distorsione è indicata con $$\mathbb{B}[\hat \Theta]=bias(\hat \Theta)=\mathbb{E}[\hat \Theta]-\Theta$$Come detto più volte la media campionaria e la varianza campionaria sono stimatori corretti.
# Mean Square Error
Lo scarto quadratico medio è un oggetto definito così $$MSE_{\theta}(\hat \Theta)=\mathbb{E}[(\hat \Theta -\Theta)^2]$$ed è una misura della dispersione dei dati o dell'errore tra un insieme di valori osservati e i valori previsti da un modello o da una stima. È comunemente utilizzato  per valutare quanto bene un modello si adatti ai dati reali; quindi quanto $\hat \Theta$ si disperde intorno a $\Theta$.
### Osservazione con Varianza
Attenzione che non vale che $MSE(\hat \Theta)\not=Var(\hat \Theta)$ poiché la varianza è definita col valore atteso e non con il valore vero. La [[1. Richiami di Probabilità#Varianza|Varianza]] è così definita: $$Var(\hat \Theta)=\mathbb{E}\big[(\hat \Theta - \mathbb{E}[\Theta])^2\big]$$Però! Se $\hat \Theta$ è non distorto per $\Theta$ allora vale proprio che $$MSE(\hat \Theta)=Var(\hat \Theta)$$## Bilanciamento fra Bias e Varianza

Abbiamo detto prima che non vale che $MSE(\hat \Theta)\not=Var(\hat \Theta)$ ma se inseriamo opportunamente il bias allora possiamo creare la seguente uguaglianza: $$MSE(\hat \Theta)=Var(\hat \Theta)+\big(bias(\hat \theta)\big)^2$$Ora va dimostrato e premetto che farò una dimostrazione con passaggi molto elementari ma siccome mi perdo sempre tornerà utile. $$\begin{align}
1. &\quad MSE(\hat \Theta)=\mathbb{E}[(\hat \Theta - \Theta)^2]\\
2. &\quad =\mathbb{E}[(\hat \Theta- \mathbb{E}[\hat \Theta] +\mathbb{E}[\hat \Theta] - \Theta)]\\
3. &\quad =\mathbb{E}[(x+y)^2]\text{con } x=\hat \Theta- \mathbb{E}[\hat \Theta]\text{ e }y=+\mathbb{E}[\hat \Theta] - \Theta =bias(\Theta) \\
4.  &\quad =\mathbb{E}(x^2 +2xy +y^2)\\
5.  &\quad =\mathbb{E}\big[(\hat \Theta- \mathbb{E}[\hat \Theta])^2 + 2*(\hat \Theta- \mathbb{E}[\hat \Theta])*bias(\hat \Theta)+(bias(\Theta))^2 \big]\\
6.  &\quad =\mathbb{E}[(\hat \Theta-\mathbb{E}[\hat \Theta])^2] + (bias(\theta))^2 + 2*(\mathbb{E}[\hat \Theta]- \mathbb{E}[\hat \Theta])*bias(\hat \Theta)\\
7. &\quad =\mathbb{E}[(\hat \Theta-\mathbb{E}[\hat \Theta])^2] + (bias(\theta))^2\\
8. &\quad = Var(\hat \Theta) + (bias(\theta))^2
   \end{align}$$Qui una spiegazione è doverosa che sono stati fatti un paio di passaggi sottili. Al $2$ si è semplicemente addizionato e sottratto lo stesso oggetto, ossia $\mathbb{E}[(\hat \Theta)]$, che torna comodo per la dimostrazione. Al passaggio $6$ si può notare che il doppio prodotto è mutato poiché è stata sfruttata la proprietà di linearità del valore atteso che è fatta così:$$\mathbb{E}[a+bX]=a+b\mathbb{E}[X]$$Da notare che siccome $\hat \theta$ è l'unico oggetto che non conosco non posso portarlo fuori con la proprietà di linearità e quindi quando esco dal valore atteso generale che ho nel $5$ devo per forza applicare l'operatore $\mathbb{E}[\bullet]$ alla $\hat \Theta$ presente nel doppio prodotto. 

## Standard Error
Se $\hat \Theta$ è corretto per $\Theta$, l'errore di stima sarà controllato dalla sola varianza di $\hat \Theta$. Chiamo standard error la seguente quantità: $$se(\hat \theta)=\sqrt{Var(\hat \theta)}$$
### Esempio
Siano $\{X_1,...,X_n\}$ iid come $X\sim Ber(\pi)$. La [[2.1 Variabili Casuali Notevoli#Bernoulli|Bernoulli]] ha $\mathbb{E}[X]=\pi$ e $Var(X)=\pi(1-\pi)$. 
$$X=I_S=\begin{cases}
1 & \text{successo}=S\\
0 & \text{insuccesso}=\bar S
\end{cases}$$
Qui al posto di $\hat \Theta$ uso $\hat \pi$ perché siamo con una Bernoulli ed è giusto usare il cappello perché è stimatore corretto per $\pi$. $$\bar X_n=\sum_{i=1}^nI_s(x_i)=\hat \pi$$Questa quantità oltre che media campionaria posso chiamarla *proporzione campionaria*. Quindi posso dire che $\mathbb{E}[\hat \pi]= \pi$ e anche che $Var(\pi)={Var(X)\over n}={\pi(1-\pi)\over n}$. A questo punto posso definire lo standard error come visto sopra e aggiungiamo che possiamo approssimarlo a $$se(\hat \pi)=\sqrt{{\hat \pi(1-\hat \pi)\over n}}$$Attenzione che la parabola disegnata da $\pi(1-\pi)$ ha massimo in $1/2$ e vale $1/4$ quindi posso definire una stima per eccesso dello standard error definita così:$$se(\hat \pi)\le \sqrt{{1\over 4n }}={1\over 2\sqrt{n}}$$
