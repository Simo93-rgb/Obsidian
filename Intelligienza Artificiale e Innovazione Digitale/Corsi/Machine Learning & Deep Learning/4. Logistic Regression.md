Anche se c'√® la parola regressione non √® davvero un modello di regressione ma √® un modello di classificazione. Il nome fuorviante arriva dal fatto che eredita delle caratteristiche dalla regressione lineare "vera". 
Classificare significa dividere in gruppi ognuno con delle caratteristiche sue (classi). 
Non va bene interpolare con una retta e fare regressione lineare, altrimenti l'aggiunta di valori come la ‚ùå in estrema destra porterebbe a ottenere una retta che non funziona pi√π bene.
![[Machine Learning - Logistic Regression - logic regression.png]]

Logic Regression: $0 \leq h_\Theta(x) \leq 1$ dove per√≤ ora l'ipotesi √® fatta cos√¨: $$h_\Theta(x) = g(\Theta^Tx)$$dove la $g$ viene detta funzione logistica. $$g(z) = {{1}\over 1+e^{-z}}$$
![[Machine Learning - Logistic Regression  - Funzione sigmoide (logistica).png|450]]

Quindi, esplicitando io la funzione di ipotesi in questo modello l'ho fatta cos√¨:$$h_\Theta(x) = g(z) = {{1}\over 1+e^{-\Theta^Tx}}$$
ed √® la probabilit√† che il mio esempio sia positivo. 
$h_\Theta(x) = \mathbb{P}(y=1|x;\Theta)$ si legge come la probabilit√† che $y$ sia $1$ dato $x$ parametrizzato da $\Theta$. 

## Logic Regression: Decision Boundary
Osservando la funzione sigmoide possiamo intuire che per avere valori positivi devo avere che $\Theta^T x \geq 0$. 
Se posso separare le classi con una retta allora si dice che le classi sono linearmente separabili. ![[Machine Learning - Logistic Regression  - classi linearmente separabili.png]]
Avendo $\Theta_0=-3; \Theta_1 = 1; \Theta_2=1$ e avendo $h_\Theta(x)=g(\Theta_0+\Theta_1x_1+\Theta_2x_2)$ avrei che la retta in verde √® data da $\Theta^Tx=-3+x_1+x_2$.
Quindi: $$\text{positive region}(‚ùå):= \;\;\;\; x_1+x_2\geq3$$
$$\text{negative region}(üîµ):= \;\;\;\; x_1+x_2\leq3$$
Posso anche fare modelli che disegnano aree anche molto contorte ma non vanno bene questi modelli perch√© si genera *overfitting*.
![[Machine Learning - Logistic Regression  - modello eccessivo.png|305]]

## Logic Regression: Funzione di Costo
Nella regressione lineare la funzione di costo abbiamo visto essere convessa e quindi per definizione ha un solo minimo. Qui invece purtroppo non lo √® e quindi posso rimanere incagliato in un minimo locale.$$Cost(h_\theta(x),y)={1\over2}(h_\Theta(x)-y)^2$$
Esiste una funzione alternativa di costo che √® convessa e quindi posso tornare ad avere garantito il minimo globale. $$Cost(h_\theta(x),y)=\begin{cases}-log(h_\Theta(x)) \;\;\text{se }y=1\\-log(1-h_\Theta(x)) \;\;\text{se }y=0 \end{cases}$$
![[Machine Learning - Logistic Regression  - costo y=1.png|400]]
![[Machine Learning - Logistic Regression  - costo y=0 classificazione logica.png|400]]
Questa funzione si chiama *cross entropy loss*. 
Lo scopo √® sempre trovare $\Theta$ che minimizzi la situazione. $$J(\Theta)={1\over m}\sum_{i=1}^m Cost(h_\Theta(x^i),y)$$
Per poter minimizzare la funzione applico la [[0. Machine Learning#Discesa del Gradiente|discesa del gradiente]] tenendo in considerazione le stesse cose viste precedentemente. 
Per ottenere il minimo: $$min_\Theta J(\Theta) = repeat\{\Theta_j = \Theta_j -\alpha \frac{\partial}{\partial \Theta_j}J(\Theta) \}$$
$$min_\Theta J(\Theta) = repeat\{\Theta_j = \Theta_j -\alpha \sum_{i=1}^m (h_\Theta(x^i)-y^i)x^i\}$$
![[Machine Learning - Logistic Regression  - Logic vs Linear regression.png]]
Questi algoritmi vanno sempre presi dalle API offerte da tensorflow e compagnia. 

## Multi-class Classification
E se non mi accontento pi√π di classificazione binaria? Qui si intende con multi un numero di classe maggiore di 2. Con tante classi possiamo mappare le classi in numeri naturali senza problemi. 
![[Machine Learning - Logistic Regression  - 2 classi vs Multi-Class.png]]

Non si pu√≤ applicare direttamente la classificazione vista prima, dove la metto la retta? Ovviamente perde di senso. Posso usare una tecnica speciale per√≤. 

### Multi-class Classification: One-vs-All
![[Machine Learning - Logistic Regression  - One VS All.png]]

Qui si restituisce il valore massimo ottenuto dal classificatore binario. Il numero di classificatori corrisponde al numero di classe. Siccome ho "uno contro tutti" si finisce per avere ogni modello che si addestra su classi sbilanciate. 

### Multi-class Classification: One-vc-One
Gioco tutti contro tutti singolarmente. $a,b,c \rightarrow a\;vs\;b;a\;vs\;c;b\;vs\;c$. Il numero di classi qui √®: $$\#class = \frac {k(k-1)}{2}$$
