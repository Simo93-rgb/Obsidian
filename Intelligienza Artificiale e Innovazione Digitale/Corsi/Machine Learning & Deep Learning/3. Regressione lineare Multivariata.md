Adesso $h_\Theta$ è diverso e fatto così:$$h_{\Theta}(x)=\Theta_0+\Theta_1x1+...+\Theta_nxn$$
Adesso la variabile $x$ e i parametri $\Theta$ sono vettori di egual dimensione
$$x =\begin{equation}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
\quad \Theta =
\begin{bmatrix}
\Theta_1 \\
\Theta_2 \\
\vdots \\
\Theta_n
\end{bmatrix}
\end{equation}
$$
 Da notare che devo fare la trasposta per poter moltiplicare i vettori, quindi $$h_\Theta(x) = \Theta^T(x)$$
 Questa è la faccia del nuovo algoritmo
 ![[Machine Learning - Regressione lineare Multivariata - discesa gradiente multiariato.png]]
 Lo si fa fino ad $n$.
## Discesa del gradiente: feature scaling
Se la scala di dimensione fra le features schiaccia la funzione c'è il rischio che l'algoritmo sia lento ![[Machine Learning - Regressione lineare Multivariata  - feature scaling non scalato.png|400]]
Quindi conviene riscalare per ottenere una funzione non schiacciata. ![[Machine Learning - Regressione lineare Multivariata  - riscalatura delle features.png|400]]
### Mean Normalisation
![[Machine Learning - Regressione lineare Multivariata  - riscalare.png]]
Lavorando così funziona bene per riscalare. 

### Learning Rate
Non esiste una regola standard per sceglierla ma in sostanza devo comunque avere un abbassamento della funzione di costo fino al raggiungimento della soglia di decrescita impostata da noi. Un modo di cercare un valore opportuno è il metodo Grid Search.

### Regressione Polinomiale
Alcune features potrebbero essere quadratiche, cubiche e quindi in generale polinomiali; ad esempio un'area o un volume. 
![[Machine Learning - Regressione lineare Multivariata  - Regressione Polinomiale.png]]
Ma comunque $h_\Theta$ lo calcolo nello stesso modo! 
Un esempio può essere questo ![[Machine Learning - Regressione lineare Multivariata  - housing price.png]]