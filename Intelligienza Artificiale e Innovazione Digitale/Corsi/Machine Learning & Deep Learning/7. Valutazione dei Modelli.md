# Diagnostic and Evaluation

## Model Selection
Divido il dataset in 3 insiemi e imparo sul *training set*, testo tutti i modelli ottenuti sul *validation set* e seleziono il modello migliore che andrò infine a testare sul *test set*. Questo se non sono sicuro di che grado di modello voglio, se ad esempio avessi una imposizione ad un modello di grado 2 allora l'operazione di *validation* non serve. Oppure, se avessi l'iperparametro lambda potrei usare questa tecnica per trovale il valore ottimale. 

### Cross Validation
È una tecnica più valida dello split netto del dataset. Identifico un valore *k-fold* che divide in parti uguali il dataset, uno standard è $k=10$ quindi avrò il dataset divisio in 10 fold. Ad esempio, addestro su 9 fold e testo su uno, l'operazione la eseguo $k$ volte e ogni volta ho un fold differente di test. Così tutti i dati vengono usati $k$ volte per test e $k-1$ volte per training. *Il test è sempre e solo su un fold*. 

#### Leave-One-Out (LOO) o Jack Knife
È un caso speciale di cross validation, se ho 100 elementi farò 100 addestramenti dove lascio fuori un solo elemento come test.  
Ottima per dataset piccoli, attenzione che su dataset grandi sono un mare di calcoli e una k-fold cross validation sarebbe consigliata. 

## Bias vs Varianza
Il bias del modello corrisponde ad *underfitting* mentre la varianza a *overfitting*. 
![[Machine Learning - Practical Machine Learning - bias vs variance.png|450]]

Plottando cosa succede in training e in test aumentando il grado del modello ottengo questo ![[Machine Learning - Practical Machine Learning  - plot test e training aumentando grado modello.png|450]]
Bisogna stare molto attenti perché un modello troppo complicato "disimpara" a generalizzare e in test commette più errori.
Il caso di underfitting lo si può vedere quando l'errore di test e l'errore di training sono grandi e magari anche una bassa distanza fra loro, mentre il caso di overfitting lo si vede quando la distanza fra gli errori è alta e in particolare l'errore di training è basso. 

## Regolarizzazione bias e varianza
Adatto $\lambda$ nel $J(\Theta)$, visto alla lezione 5. Fa parte di quei valori da trovare sperimentalmente con *grid search* piuttosto che *baesyan optimization*.

## Learning Curves
Aumentare il numero di esempi aumenta l'errore di training ma diminuirà l'errore in fase di test. ![[Machine Learning - Practical Machine Learning  - curva errore test e training.png]]
Attenzione che se c'è un alto bias aumentare la dimensione non aiuta da sola, poiché l'errore di training rimane troppo alto. Se invece ho un alta varianza conviene aumentare la dimensione del dataset. Quindi:
- underfitting $\rightarrow$ non serve aumentare la dimensione del dataset
- overfitting $\rightarrow$ serve aumentare la dimensione del dataset

## Decidere cosa provare dopo

Supponiamo di aver implementato la regressione lineare regolarizzata per prevedere i prezzi delle case. Tuttavia, quando testiamo l'ipotesi su un nuovo set di case, scopriamo che commette errori inaccettabilmente grandi nelle sue previsioni. Cosa dovremmo provare a fare dopo?​

- Ottenere più esempi di addestramento​ $\rightarrow$ risolve l'alta varianza
- Provare con set di feature più piccoli​ $\rightarrow$ risolve l'alta varianza
- Provare a ottenere feature aggiuntive​ $(x_1^2,x_2^2,x_1,x_2...)$ $\rightarrow$ risolve l'alto bias
- Provare ad aggiungere feature polinomiali​ $\rightarrow$ risolve l'alto bias
- Provare a diminuire​ $\lambda$ $\rightarrow$ risolve l'alto bias
- Provare ad aumentare​ $\lambda$ $\rightarrow$ risolve l'alta varianza

Nelle reti neurali ho una architettura a priori (layer, neuroni...) che se è piccolina avrà un alto bias mentre una rete molto grande invece avrà un'alta varianza. È intuitivo, infatti una rete molto articolata sarà molto brava a fittare perfettamente i dati ma abbiamo già visto che significa, appunto, alta probabilità di overfitting. 

### Confusion Matrix
![[Machine Learning - Practical Machine Learning  - confusion matri.png]]
La matrice di confusione rappresenta la mia classe e come io l'ho classificata, lo abbiamo visto a statistica nel Montecarlo per inferenza. 

| Realtà/Criterio | Accetto $H_0$   | Rifiuto $H_0$  |
| --------------- | --------------- | -------------- |
| $H_0$ vera      | OK              | Errore I° tipo |
| $H_0$ falsa     | Errore II° tipo | OK             |

![[Machine Learning - Practical Machine Learning - recall specifity etc.png]]

Dove:
- TPR $\rightarrow$ True Positive Rate o *Recall* (*Sensitivity* o Sensibilità)
- TNR $\rightarrow$ True Negative Rate o Specificità (*Specifity*)
- FPR $\rightarrow$ False Positive Error o *Fallout*
- FNR $\rightarrow$ False Negative Rate

#### Accuratezza
Dalla matrice di confusione posso costruire l'**accuratezza** $$a={{TP+TN}\over m}$$ossia la percentuale di corretta classificazione. Attenzione che una accuratezza elevata non significa che il modello sia buono e bisogna testare anche altro, ma se è bassa allora lo si può buttare. 
#### Cohen's Kappa Coefficient
$$k={{a-p_e}\over{1-p_e}}$$dove $p_e=(p_+ + p_-)$ con $p_+$ la probabilità di prenderci per caso sui casi positivi e rispettivamente coi negativi con $p_-$. Se $k$ tende a zero allora l'accuratezza non è significativa, mentre se tende a 1 allora è significativa. 
$$\begin{align}
&\quad p_+={{TP+FN\over m}*{TP+FP\over m}}\\
&\quad p_-= {{TN+FN\over m}*{TN+FN\over m}}
\end{align}$$
### Esempio
![[Machine Learning - Practical Machine Learning  - esempio cohen kappa coeff..png]]
Se la matrice è asimmetrica si dice che le classi sono **sbilanciate** e sicuramente l'accuratezza non sarà significativa. Esistono tecniche artificiali di ribilanciamento delle classi ma non lo vedremo.

## ROC Curve
Receiver Operating Characteristic è una curva plottata su un quadrante con ascisse che rappresentano il *fallout* e sulle ordinate la *recall*. ![[Machine Learning - Practical Machine Learning  - ROC curve.png]]
- $(0,1)$ perfect classification​    
- $(0,0)$ always class –​    
- $(1,1)$ always class +     
- $(1,0)$ + classified as – and viceversa​
La distanza dal punto $(1,0)$ è la discriminante per dire che un classificatore $C$ sia meglio di un classificatore $A$. 
Se cambio gli iperparametri e plotto il punto del clasificatore $A$ con iparametri differenti avrò una curva. Posso quindi mettere a paragone curve diverse per classificatori diversi a iperparametri differenti. ![[Machine Learning - Practical Machine Learning  - ROC Curve comparazione.png]]
L'area sotto la curva ROC, ossia il parametro AUC (Area Under the Curve), può valere massimo 1, più alto il valore migliore il classificatore. C'è una digressione matematica vista alla lezione di statistica parlando di inferenza con Montecarlo. 

## Error Metric for Skewed Classes
Definiamo la precisione: $$prec = {TP\over TP+FP}$$Questa è l'intersezione sotto in figura, ossia quanti positivi sul totale dei classificati positivi. ![[Machine Learning - Practical Machine Learning  -  precision.png|300]]
Richiamiamo il concetto di recall visto prima nella [[0. Machine Learning#Confusion Matrix|Matrice di Confusione]], a parole è il caso dei casi misurati positivi sul totale dei casi positivi. 
![[Machine Learning - Practical Machine Learning  - comparazione falsi positivi e falsi negativi.png|450]]
Se alzo le predizioni dei falsi positivi abbasso i falsi negativi, ad esempio nel caso di una diagnosi di tumore mi conviene far prendere un coccole ad un falso positivo ma è imperativo non perdersi per strada i falsi negativi che hanno bisogno di cure. Quindi se aumento precision ho meno recall e vice versa. 
Posso fare una curva fra precision e recall e calcolare l' area sotto la curva.![[Machine Learning - Practical Machine Learning  - curva PR.png]]Attenzione che punto iniziale e finale non si riescono sempre ad avere perché precision e recall che valgono esattamente 1 o molto vicini ma si usa un trucco, si proietta linearmente dal punto più vicino fino all'asse.
### Confronto: Precisione, Recall, Media e F1 Score

| Algorithm   | Precision (P) | Recall (R) | Average | F1 Score |
| ----------- | ------------- | ---------- | ------- | -------- |
| Algorithm 1 | 0.5           | 0.4        | 0.45    | 0.444    |
| Algorithm 2 | 0.7           | 0.1        | 0.4     | 0.175    |
| Algorithm 3 | 0.02          | 1.0        | 0.51    | 0.0392   |

**Media (Average):** $$\text{Average} = \frac{P + R}{2}$$

**F1 Score (Media Armonica):** 
$$F_1 = 2 \cdot \frac{P \cdot R}{P + R}$$ 
Il **F1 Score** è la **media armonica** tra la precisione e il recall. È particolarmente utile quando cerchiamo un compromesso tra precisione e recall, poiché dà uguale importanza a entrambi.

- Se $F_1 = 0$ allora $\text{Precision} = 0$ oppure $\text{Recall} = 0$.
- Se $F_1 = 1$ allora $\text{Precision} = 1$ e $\text{Recall} = 1$.
Il **Fβ Score** è una generalizzazione del F1 Score. Il parametro $\beta$ permette di dare più importanza alla precisione o al recall, a seconda del valore scelto per $\beta$. Se $\beta = 1$ allora degenera nella media armonica vista qui sopra.
 $F_\beta \,Score$:  $$F_\beta=(1+\beta)^2{prec*rec\over \beta^2*prec+rec}$$
#### Considerazioni:
- Il miglior bilanciamento tra **Precision** e **Recall** è rappresentato dal **F1 Score**.
- L'algoritmo che ottiene il miglior compromesso è l'**Algorithm 1** con un F1 Score di 0.444.
