# Diagnostic and Evaluation

## Model Selection
Divido il dataset in 3 insiemi e imparo sul *training set*, testo tutti i modelli ottenuti sul *validation set* e seleziono il modello migliore che andrò infine a testare sul *test set*. Questo se non sono sicuro di che grado di modello voglio, se ad esempio avessi una imposizione ad un modello di grado 2 allora l'operazione di *validation* non serve. Oppure, se avessi l'iperparametro lambda potrei usare questa tecnica per trovale il valore ottimale. 

### Cross Validation
È una tecnica più valida dello split netto del dataset. Identifico un valore *k-fold* che divide in parti uguali il dataset, uno standard è $k=10$ quindi avrò il dataset divisio in 10 fold. Ad esempio, addestro su 9 fold e testo su uno, l'operazione la eseguo $k$ volte e ogni volta ho un fold differente di test. Così tutti i dati vengono usati $k$ volte per test e $k-1$ volte per training. *Il test è sempre e solo su un fold*. 

#### Leave-One-Out (LOO)
È un caso speciale di cross validation, se ho 100 elementi farò 100 addestramenti dove lascio fuori un solo elemento come test.  
Ottima per dataset piccoli, attenzione che su dataset grandi sono un mare di calcoli e una k-fold cross validation sarebbe consigliata. 

## Bias vs Varianza
Il bias del modello corrisponde ad *underfitting* mentre la varianza a *overfitting*. 
![[Machine Learning - Practical Machine Learning - bias vs variance.png|450]]

Plottando cosa succede in training e in test aumentando il grado del modello ottengo questo ![[Machine Learning - Practical Machine Learning  - plot test e training aumentando grado modello.png|450]]
Bisogna stare molto attenti perché un modello troppo complicato "disimpara" a generalizzare e in test commette più errori.
Il caso di underfitting lo si può vedere quando l'errore di test e l'errore di training sono grandi e magari anche una bassa distanza fra loro, mentre il caso di overfitting lo si vede quando la distanza fra gli errori è alta e in particolare l'errore di training è basso. 

## Regolarizzazione bias e varianza
Adatto $\lambda$ nel $J(\Theta)$ 

## Learning Curves
Aumentare il numero di esempi aumenta l'errore di training ma diminuirà l'errore in fase di test. ![[Machine Learning - Practical Machine Learning  - curva errore test e training.png]]
Attenzione che se c'è un alto bias aumentare la dimensione non aiuta da sola, poiché l'errore di training rimane troppo alto. Se invece ho un alta varianza conviene aumentare la dimensione del dataset. Quindi:
- underfitting $\rightarrow$ non serve aumentare la dimensione del dataset
- overfitting $\rightarrow$ serve aumentare la dimensione del dataset

## Deciding what to try next
Suppose you have implemented regularized linear regression to predict housing prices. However, when you test your hypothesis in a new set of houses, you find that it makes unacceptably large errors in its prediction. What should you try next? ​

- Get more training examples​ $\rightarrow$ fixes high variance    
- Try smaller sets of features​ $\rightarrow$ fixes high variance    
- Try getting additional features​ $(x_1^2,x_2^2,x_1,x_2...)$ $\rightarrow$ fixes high bias    
- Try adding polynomial features​ $\rightarrow$ fixes high bias    
- Try decreasing​ $\lambda$ $\rightarrow$ fixes high bias    
- Try increasing​ $\lambda$ $\rightarrow$ fixes high variance

Nelle reti neurali ho una architettura a priori (layer, neuroni...) che se è piccolina avrà un alto bias mentre una rete molto grande invece avrà un'alta varianza. È intuitivo, infatti una rete molto articolata sarà molto brava a fittare perfettamente i dati ma abbiamo già visto che significa, appunto, alta probabilità di overfitting. 

### Confusion Matrix
![[Machine Learning - Practical Machine Learning  - confusion matri.png]]
La matrice di confusione rappresenta la mia classe e come io l'ho classificata, lo abbiamo visto a statistica nel Montecarlo per inferenza. ![[Machine Learning - Practical Machine Learning  - confusion matrix MC inferenza.png]]

![[Machine Learning - Practical Machine Learning - recall specifity etc.png]]
Dove:
- TPR $\rightarrow$ True Positive Rate o *Recall* (*Sensitivity* o Sensibilità)
- TNR $\rightarrow$ True Negatove Rate o Specificità (*Specifity*)
- FPR $\rightarrow$ False Positive Error o *Fallout*
- FNR $\rightarrow$ False Negative Rate

Dalla matrice di confusione posso costruire l'**accuratezza** $$a={{TP+TN}\over m}$$ossia la percentuale di corretta classificazione. Attenzione che una accuratezza elevata non significa che il modello sia buono e bisogna testare anche altro, ma se è bassa allora lo si può buttare. 
**Cohen's Kappa Coefficient** $$k={{a-p_e}\over{1-p_e}}$$dove $p_e=p_++p_-$ con $p_+$ è la probabilità di prenderci per caso sui casi positivi e rispettivamente coi negativi con $p_-$. Se $k$ tende a zero allora l'accuratezza non è significativa, mentre se tende a 1 allora è significativa. 
$$\begin{align}
&\quad p_+={{TP+FN\over m}*{TP+FP\over m}}\\
&\quad p_-= {{TN+FN\over m}*{TN+FN\over m}}
\end{align}$$
### Esempio
![[Machine Learning - Practical Machine Learning  - esempio cohen kappa coeff..png]]
Se la matrice è asimmetrica si dice che le classi sono **sbilanciate** e sicuramente l'accuratezza non sarà significativa. Esistono tecniche artificiali di ribilanciamento delle classi ma non lo vedremo.

## ROC Curve
Receiver Operating Characteristic è una curva plottata su un quadrante con ascisse che è FPR e sulle ascisse TPR. ![[Machine Learning - Practical Machine Learning  - ROC curve.png]]
- $(0,1)$ perfect classification​    
- $(0,0)$ always class –​    
- $(1,1)$ always class +     
- $(1,0)$ + classified as – and viceversa​
La distanza dal punto $(1,0)$ è la discriminante per dire che un classificatore $C$ sia meglio di un classificatore $A$. 
Se cambio gli iperparametri e plotto il punto del clasificatore $A$ con parametri differenti avrò una curva. Posso quindi mettere a paragone curve diverse per classificatori diversi a iperparametri differenti. ![[Machine Learning - Practical Machine Learning  - ROC Curve comparazione.png]]
L'area sotto la curva ROC, ossia il parametro AUC (Area Under the Curve), può valere massimo 1, più alto il valore migliore il classificatore. C'è una digressione matematica vista alla lezione di statistica parlando di inferenza con Montecarlo. 

## Error Metric for Skewed Classes
Definiamo la precisione $$prec = {TP\over TP+FP}$$ ed è l'intersezione in figura, ossia quanti positivi sul totale dei classificati positivi. ![[Machine Learning - Practical Machine Learning  -  precision.png|300]]
Richiamiamo il concetto di recall visto prima nella [[0. Machine Learning#Confusion Matrix|Matrice di Confusione]], a parole è il caso dei casi misurati positivi sul totale dei casi positivi. 
![[Machine Learning - Practical Machine Learning  - comparazione falsi positivi e falsi negativi.png|450]]
Se alzo le predizioni dei falsi positivi abbasso i falsi negativi, ad esempio nel caso di una diagnosi di tumore mi conviene farlo così non rischio di non dare le cure ai malati perché falso negativo al prezzo di far prendere un coccolone al falso positivo. Quindi se aumento precision ho meno recall e vice versa. 
Posso fare una curva fra precision e recall e calcolare l' area sotto la curva.![[Machine Learning - Practical Machine Learning  - curva PR.png]] Attenzione che punto iniziale e finale non si riescono sempre ad avere perché precision e recall che valgono esattamente 1 o molto vicini ma si usa un trucco, si proietta linearmente dal punto più vicino fino all'asse.
#### Confronto
![[Machine Learning - Practical Machine Learning  - f1 score.png]]
$F_1 \,Score$ è anche chiamato media armonica. Esiste anche $F_\beta \,Score$: $$F_\beta=(1+\beta)^2{prec*rec\over \beta^2*prec+rec}$$ dove $\beta$ è il peso dell'importanza relativa di rev e prec dove con valori maggiori di 1 do importanza a rec, minore do importanza a prec. Se $\beta = 1$ allora è esattamente $F_1\,Score$.
