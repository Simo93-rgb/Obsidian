Devo cercare la miglior retta che interpola i dati; nemmeno deve venire in mente di cercare una curva. 
![[Machine Learning - Regressione Lineare Univariata - regressione univariata.png]]

Un po' di notazione:
- m $\rightarrow$ dimensione training set
- x $\rightarrow$ variabili/features
- y $\rightarrow$ output/target
- $(x,y)$ $\rightarrow$ coppia di addestramento singolo
- $(x^i, y^i)$ $\rightarrow$ i-esimo addestramento

Quindi, devo cercare di costruire il modello qui sotto dove con *h* indico l'ipotesi
![[Machine Learning - Regressione Lineare Univariata  - schema addestramento.png]]

La $h$ deve essere qualcosa che mi permette di trovare la migliore retta possibile. 
![[Machine Learning - Regressione Lineare Univariata  - trovare h come equazione della retta.png]]

## Funzione di costo
![[Machine Learning - Regressione Lineare Univariata  - richiami retta.png]]
Un ripasso sull'equazione della retta. $\Theta_0$ è l'intercetta. 
**Idea!** Trovare un valore di $\Theta_0$ e di $\Theta_1$ che permetta di avere $h_0(x)$ vicino a $y$ per il nostro set di addestramento $(x,y)$. 
$$J(\Theta_0,\Theta_1)={1\over2m}\sum_{i=1}^m\big(h_{\Theta}(x^i)-y^i\big)^2$$
*Quindi, se faccio lo scarto quadratico medio ottengo una funzione che tanto più ha valori piccoli, meglio è!*
Ricordiamo che $\Theta_0(x^i)$ è il valore predetto.
Questo è l'obbiettivo di tutti i modelli di ML:
$$min[J(\Theta_0,\Theta_1)]$$
Tradotto, minimizzare la *loss*. Ovviamente ci sono dei metodi come la discesa del gradiente che vedremo più avanti.
La funzione di costo ha una forma parabolica centrata sul coefficiente angolare della retta migliore. La funzione di costo qui nella regressione lineare è convessa e per definizione ha un solo minimo. 
![[Machine Learning - Regressione Lineare Univariata  - grafico funzione di costo.png|500]]
In sintesi:
* Ipotesi $\rightarrow$ è $h_\Theta(x)$
* Parametri $\rightarrow$ $\Theta_0$ e $\Theta_1$
* Funzione di costo $\rightarrow$ $J(\Theta_0,\Theta_1)$
* Goal $\rightarrow$ minimizzare la funzione di costo

![[Machine Learning - Regressione Lineare Univariata  - funzione di costo della retta.png]]

guardando l'immagine sopra, le coordinate del punto sono $\Theta_0,\Theta_1$ e il punto sta dentro ad un'area che rappresenta l'insieme dei "lenzuoli" come quello sotto. 
![[Machine Learning - Regressione Lineare Univariata  - lenzuolo.png]]

## Discesa del Gradiente

Il gradiente è il vettore delle derivate parziali e la dimensione del vettore corrisponde al numero di variabili della funzione. Permette di simulare la discesa lungo la curva della funzione di costo fino a raggiungere il minimo; ricordiamo che il minimo non è locale ma globale siccome è una funzione convessa. 
È come l'algoritmo di [[1.4 Iterative Improvement (Local) search#Hill-climbing|Hill Climing]]. 
L'algoritmo è fatto così:
![[Machine Learning - Regressione Lineare Univariata  - Algoritmo discesa del gradiente.png]]
Da notare che $\alpha$ è un *iperparametro* che conosciamo come **learning rate**. In generale, se è un iperparametro significa che definisce l'algoritmo, mentre se è parametro allora definisce il modello. 
![[Machine Learning - Regressione Lineare Univariata  - learning rate grafico.png|400]]
Ecco come avanza il learning rate. Questo iperparametro non va modificato durante l'avanzamento dell'algoritmo poiché si riduce da solo il passo in quanto cambia la pendenza man mano che ci si avvicina al valore centrato nella curva. Si osserva che un valore bassissimo porta sicuramente al valore, se c'è, ma costa tanto. Mentre troppo grande potrebbe non portare mai al minimo. 
Questo algoritmo si trova anche col nome di batch gradient descend poiché il concetto è che bisogna "ripassare" tutto il training set ad ogni operazione. 



