Adesso $h_\Theta$ è diverso e fatto così:$$h_{\Theta}(x)=\Theta_0+\Theta_1x1+...+\Theta_nxn$$
*Notazione: online spesso i $\Theta$ sono chiamati pesi e indicati con $w$.*
Adesso la variabile $x$ e i parametri $\Theta$ sono vettori di uguale dimensione
$$x =\begin{equation}
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
\quad \Theta =
\begin{bmatrix}
\Theta_1 \\
\Theta_2 \\
\vdots \\
\Theta_n
\end{bmatrix}
\end{equation}
$$
 Da notare che devo fare la trasposta per poter moltiplicare i vettori, la dimensione colonna del primo deve coincidere con la dimensione riga del secondo, quindi $$h_\Theta(x) = \Theta^T(x)$$
 Questa è la faccia del nuovo algoritmo
 
$$\begin{align*}
\theta_0 &:= \theta_0 - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_0^{(i)} \\
\theta_1 &:= \theta_1 - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_1^{(i)} \\
\theta_2 &:= \theta_2 - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_2^{(i)} \\
\vdots \\
\theta_n &:= \theta_n - \alpha \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_n^{(i)} \\
\end{align*}
$$

## Discesa del gradiente: feature scaling
Se la scala di dimensione fra le features schiaccia la funzione c'è il rischio che l'algoritmo sia lento ![[Machine Learning - Regressione lineare Multivariata  - feature scaling non scalato.png|400]]
Quindi conviene riscalare per ottenere una funzione non schiacciata. ![[Machine Learning - Regressione lineare Multivariata  - riscalatura delle features.png|400]]
### Mean Normalisation

$$
X_i \leftarrow \frac{X_i - \mu_i}{S_i}
$$

- $\mu_i$ è il valore medio di $X_i$.
- $S_i$ è la deviazione standard o l'intervallo medio di $X_i$.

Lavorando così funziona bene per riscalare. 

### Learning Rate
Non esiste una regola standard per sceglierla ma in sostanza devo comunque avere un abbassamento della funzione di costo fino al raggiungimento della soglia di decrescita impostata da noi. Un modo di cercare un valore opportuno è il metodo Grid Search.

### Regressione Polinomiale
Alcune features potrebbero essere quadratiche, cubiche e quindi in generale polinomiali; ad esempio un'area o un volume. 
![[Machine Learning - Regressione lineare Multivariata  - Regressione Polinomiale.png]]
Ma comunque $h_\Theta$ lo calcolo nello stesso modo! 
Un esempio può essere questo ![[Machine Learning - Regressione lineare Multivariata  - housing price.png]]