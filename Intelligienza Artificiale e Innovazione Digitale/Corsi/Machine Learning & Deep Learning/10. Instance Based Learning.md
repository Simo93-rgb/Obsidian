# Lazy Learning
Viene anche chiamato *lazy learning* e il nome deriva per un diretto confronto con i metodi che abbiamo visto finora che sono tutti della famiglia degli *eager learning*. 

| **Eager Learning**                                                                         | **Lazy Learning**                                                                           |
| ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------- |
| descrizione esplicita (modello) della funzione target sull'intero set di addestramento<br> | l'apprendimento avviene memorizzando tutte le istanze di addestramento (instance-based)<br> |
| una volta costruito il modello, il traning set pu√≤ essere eliminato                        | la previsione avviene confrontando la nuova istanza con quelle memorizzate                  |
| la predizione avviene utilizzando il modello con un nuovo esempio come input               | il confronto si basa su funzioni di distanza/similarit√†                                     |
### Pro e Contro
Rispetto ad una rete neurale, ossia eager learning, la computazione √® legata alla fase di classificazione nei modelli lazy e quindi ogni volta che interrogo il mio modello utilizzo elevata computazione mentre una volta che una NN √® addestrata non viene pi√π richiesto un grande consumo di risorse per la predizione. Nei modelli lazy l'apprendimento √® soltanto responsabilit√† della base di conoscenza.   

# k-NN
√à l'approccio principale fra tutti i modelli instance-based ed √® un approccio gi√† visto nel [[2.2 Case-Based Reasoning#k-NN|CBR]] nel corso di IA, si tratta di trovare il "vicinato" di *k* elementi misurando la distanza fra features come visto in [[2.2 Case-Based Reasoning#Similarit√† e Distanza|CBR - Similarit√† e Distanza]].  
##### Classificazione
Data una query $x$ recupero i k-NN di $x$ e gli assegno la classe maggiormente rappresentata dai vicini recuperati.

##### Regressione
Data una query $x$ recupero i k-NN di $x$ e faccio la media dei valori dei vicini recuperati.

#### Distanza
Abbiamo gi√† visto i [[2.2 Case-Based Reasoning#Tipi di Distanze|tipi di distanze]] ma bisogna fare una puntualizzazione sul parametro $p$ della distanza di Minkowsky. Variare $p$ significa variare la forma dell'area su cui misuro la distanza

![[Instance Based Learning - minkowsky al variare di p.png.png]]

Al ridursi di $p$ collasso nel centro mentre all'umentare degenero in un quadrato. La scelta di un $p$ diverso da quello che mi da la distanza euclidea dipende dalla forma dei dati.

## Voronoi e Decision Boundaries
I [[2.2 Case-Based Reasoning#Diagrammi di Voronoi|diagrammi di Voronoi]] sono delle aree nelle quali se ricado sono sicuro che il nearest neighbourhood √® il punto che caratterizza l'area, chiamato anche *seed*.  ![[Instance Based Learning - Voronoi.png]]
Quando ho $k=1$ nell'algoritmo k-NN sto di fatto costruendo un diagramma di Voronoi. Quando poi $k$ assume altri valori allora devo prendere i $k$ valori pi√π vicini tracciando un'area che contenga i $k$ seed pi√π vicini. Questo approccio riduce il rumore ma complica il modello e non √® sempre facile rappresentare queste aree quando $k>1$.
![[Instance Based Learning - 3-NN.png]]<p style="text-align:center;">Caso 3-NN con 2 seed di classe ‚ùå e uno di classe üîµ</p>
### Decision Boundaries
Sono un sottoinsieme dei diagrammi di Voronoi e sono delle linee che separano due punti equidistanti. All'aumentare degli esempi la complessit√† aumenta drasticamente. ![[Instance Based Learning - Decision Boundaries.png|450]]
Come per i diagrammi di Voronoi anche qui il valore di $k$ influenza drasticamente il modello andando a rischiare l'overfitting per valori troppo bassi. ![[Instance Based Learning - effectto di k suidecision boundaries.png]]
√à anche vero che un $k$ troppo grande produce underfitting ![[Instance Based Learning - underfitting k decision boundaries.png|450]]
## Scale Effect
Attenzione che la grandezza della scala influenza tantissimo quindi bisogna sempre fare features scaling. Ci sono almeno tre approcci fondamentali ma il consigliato rimane la standardizzazione. Vediamoli. 

#### Z-Score
Anche chiamata *standardizzazione*, si tratta di portare tutto su una $\mathcal{N}(0,1)$ con la formula: $$\begin{align}
&\quad x_i^{'}={{x_i-\bar x}\over{S_x}} \\
&\quad \bar x \rightarrow \text{media campionaria di x}\\
&\quad S_x \rightarrow \text{deviazione standard campionaria di x}
\end{align}$$
√à altamente consigliato questo approccio
##### Rescaling 
Le features possono essere scalate con la seguente: $$
x_i^{'}={{x_i-min(x)}\over{max(x)-min(x)}}\in[0,1]
$$
##### Mean Normalization
Anche chiamata *zero mean*: $$
x_i^{'}={{x_i-\bar x}\over{max(x)-min(x)}}
$$
## Pesatura
Do un peso per far pesare di pi√π i vicini che hanno distanza minore dalla query rispetto a quelli pi√π lontani. Attenzione, stiamo sempre parlando dei $k$ vicini ottenuti dall'algoritmo.
Nel caso di una regressione si tratter√† quindi di fare una media pesata, nel caso di classificazione √® come un sistema di voti dove i votanti hanno importanza differente. 

#### Voto Ponderato
Come accennato posso fare un sistema di voto pesato ma prima mi serve un sistema per calcolare questi pesi. Siano $w_iq$ il peso della distanza della query dal valore vicino, $x_i$ il valore vicino e $x_q$ la query:
$$\begin{align}
&\quad w_{iq}={1\over d(x_i,x_q)^2}&\text{definizione standard}\\
&\quad w_{iq}=1-d(x_i,x_q)^2 & \text{se }d(x_i,x_q)\in[0,1]
\end{align}$$
Ottenuto il peso posso, come gi√† detto, fare la media pesata per il caso della regressione ma per la classificazione devo calcolare un *score*. Siano $x_q$ la query, $c(x)$ la classe dell'istanza $x$ ed $n$ il numero di tutte le istanze recuperate, lo score √®: $$score(c_k,x_q)={
{\sum_{i=1}^{\#(c(x_i)=c_k)}}w_{iq}
\over
{\sum_{i=1}^n}w_{iq}}$$
Dove $\#(c(x_i)=c_k)$ √® la numerosit√† delle classi che sono uguali alla classe $c_k$ di cui si vuole calcolare lo score. Quindi risulter√† il rapporto fra la somma dei pesi della classe $c_k$ e il totale dei pesi delle classi totali. 

## Richiami
Nel corso di IA abbiamo gi√† parlato di [[2.2 Case-Based Reasoning#Curse of Dimensionality|Curse of Dimensionality]] e di [[2.2 Case-Based Reasoning#Metriche su Features|Metriche su Features]] e vanno riportate anche qui. 