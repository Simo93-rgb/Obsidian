
## Introduzione

Il **Masked Label Prediction** Ã¨ una tecnica di *self-supervised learning* utilizzata principalmente per apprendere rappresentazioni significative dai dati senza fare affidamento su etichette manuali. Ãˆ una strategia particolarmente popolare nel campo del **Natural Language Processing (NLP)**, ma trova applicazione anche in ambiti come la visione artificiale e la bioinformatica.

### ðŸ§  Cos'Ã¨ il Self-Supervised Learning

Il **Self-Supervised Learning (SSL)** Ã¨ una forma di apprendimento automatico in cui il modello **impara a supervisionarsi da solo**, estraendo etichette o segnali di addestramento direttamente dai dati non etichettati.

#### ðŸ§© Come funziona?
- Si definisce un **compito pretest** (pretext task) che il modello deve risolvere.
- Le **etichette** per questo compito vengono generate **automaticamente** dai dati.
- Dopo l'apprendimento, il modello puÃ² essere **riutilizzato o fine-tuned** su altri task (downstream) supervisionati o non.

#### ðŸ“Œ Esempi di compiti pretest
- Mascherare parte dell'input e predirla (es. Masked Language Modeling)
- Predire il frame successivo in un video
- Determinare se due parti di un'immagine appartengono alla stessa scena

#### âœ… Vantaggi
- Riduce o elimina la necessitÃ  di dati etichettati
- Produce rappresentazioni generalizzabili
- Ãˆ scalabile su grandi quantitÃ  di dati grezzi

> ðŸ§ª Ãˆ spesso usato come fase di **pretraining** prima di un fine-tuning supervisionato.

## Concetto Chiave Masked Label Prediction

Nel *Masked Label Prediction*, una parte dell'input (tipicamente un token, un pixel o una regione) viene **mascherata** e il modello viene addestrato a **predire l'informazione mancante** utilizzando il contesto circostante.

Questa metodologia consente al modello di apprendere rappresentazioni ricche e contestuali senza supervisione esplicita, sfruttando solo i dati grezzi.

## Esempio: NLP (BERT)

Uno degli esempi piÃ¹ noti Ã¨ **BERT (Bidirectional Encoder Representations from Transformers)**, che utilizza il *Masked Language Modeling* (MLM):

- Una percentuale dei token in una frase viene sostituita con un token speciale `[MASK]`.
- Il modello deve predire il token originale dato il contesto bidirezionale.

Esempio:

Input: "Il gatto Ã¨ [MASK] sul tappeto."  
Target: "seduto"


## Vantaggi

- **Indipendenza dalle etichette**: non Ã¨ necessaria un'annotazione manuale dei dati.
- **Apprendimento generalizzato**: permette al modello di apprendere rappresentazioni trasferibili a molti task downstream.
- **Riduzione del costo**: ideale per scenari con grandi moli di dati non etichettati.

## Applicazioni

- Pretraining di modelli linguistici (BERT, RoBERTa)
- Apprendimento visivo (MAE - Masked Autoencoders)
- Bioinformatica (previsione di sequenze proteiche)
- Time Series Imputation

## Conclusioni

Il Masked Label Prediction Ã¨ un pilastro del *self-supervised learning*, in grado di abilitare modelli potenti partendo da dati non etichettati. La sua efficacia deriva dalla capacitÃ  di apprendere rappresentazioni semantiche profonde sfruttando semplici meccanismi di predizione su parti mascherate dellâ€™input.
