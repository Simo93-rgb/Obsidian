# Primo Scambio di Email

Ciao Simone,

scusa se mi sono preso un po' di tempo per risponderti. Sono contento che ti interessi il progetto riguardante l'utilizzo dei Transformer per la classificazione delle tracce sanitarie.

Ho già iniziato a riprendere il lavoro sul Transformer, studiando come funziona e quali sono le sue prestazioni con il nuovo dataset. L'idea è di applicare questa tecnologia alle nuove tracce provenienti dall'Ospedale di Alessandria per identificare eventuali ritardi nelle procedure mediche.

L'architettura che stiamo usando è una tipica encoder-decoder, progettata per generare token e ricostruire le azioni mancanti basandosi su un prefisso di traccia. L'implementazione originale è stata sviluppata da un dottorando ora a Oxford e ha già mostrato risultati promettenti con pazienti affetti da ictus.

Per il nuovo contesto, vorremmo classificare le tracce come "normali" o "problematiche", basandoci su potenziali colli di bottiglia che potrebbero allungare la permanenza dei pazienti in ospedale. Inoltre, un altro aspetto importante del progetto sarà quello dell'explicabilità (explainability), sfruttando le attention maps per capire quali azioni influenzano maggiormente l'assegnazione della classe.

Ti metterei volentieri a lavorare in questo contesto se sei interessato. Ci saranno molte possibilità di crescita e apprendimento.

## Competenze Necessarie

### 1. Conoscenza dei Transformers

#### Fondamenti Teorici:

- **Architettura del Transformer**: Leggi la paper originale "Attention is All You Need" di Vaswani et al. per capire l'architettura base, comprese le componenti Encoder e Decoder.
- **Self-attention meccanismo**: Studia come funziona il meccanismo di attenzione multipla e perché è centrale nei Transformers.

### 2. Elaborazione del Linguaggio Naturale (NLP) e Process Mining

#### NLP:

- **Basics of Text Preprocessing**: Leggi sulla tokenizzazione, lemmatizzazione, e stemming.
- **Sequence Modeling**: Studia l'uso dei modelli RNN, LSTM, GRU nel contesto di NLP e come sono stati superati dai Transformer.

#### Process Mining:

- **Concetti Fondamentali**: Familiarizza con termini chiave come event logs, process discovery, e conformance checking.
- **Applicazioni del Process Mining in Sanità**: Analizza come i dati delle tracce sanitarie possono essere utilizzati per scoprire modelli di processo.

### 3. Interpretazione e Spiegabilità (Explainability)

#### Teoria dell’Esplicabilità:

- **Attention Maps**: Approfondisci come le attenzioni nei Transformer possono essere utilizzate per spiegare le decisioni del modello.
- **Metodi di Spiegazione Modelli Complessi**: Studia strumenti e tecniche come LIME, SHAP per interpretare modelli complessi.

## 4. Graph Neural Networks
I GNN estendono i principi delle reti neurali tradizionali (come le CNN per immagini o le RNN per sequenze) al dominio dei grafi. L'obiettivo principale è **apprendere rappresentazioni (embedding)** per i nodi, gli archi o l'intero grafo in uno spazio vettoriale a bassa dimensionalità, preservando le informazioni strutturali e relazionali del grafo.

Queste rappresentazioni possono essere utilizzate per risolvere vari problemi, come:

1. **Classificazione dei nodi** : Assegnare etichette ai nodi (es. identificare utenti fraudolenti in una rete sociale).
2. **Classificazione del grafo** : Assegnare un'etichetta all'intero grafo (es. prevedere la tossicità di una molecola).
3. **Predizione dei link** : Prevedere se esiste un collegamento mancante tra due nodi (es. suggerire nuove amicizie in una rete sociale).
4. **Generazione di grafi** : Creare nuovi grafi basati su esempi noti (es. generare nuove molecole per scopi farmaceutici).

Per codificare informazioni strutturali in grafi esistono degli algoritmi per farlo, come ad esempio **Laplacian Eigenvector Positional Encoding** e **Random Walk Positional Encoding**.

### **Laplacian Eigenvector Positional Encoding**

Il **Laplacian Eigenvector Positional Encoding** è una tecnica che utilizza gli autovettori della matrice Laplaciana del grafo per codificare la posizione dei nodi. La matrice Laplaciana è una rappresentazione matematica del grafo che cattura le sue proprietà strutturali.

La matrice Laplaciana L di un grafo è definita come: $$L=D-A$$
dove:

- $D$ è la matrice diagonale dei gradi dei nodi (ogni elemento diagonale rappresenta il grado di un nodo),
- $A$ è la matrice di adiacenza del grafo.

Gli **autovalori** e gli **autovettori** della matrice Laplaciana contengono informazioni importanti sulla struttura del grafo. In particolare:
- Gli **autovettori** rappresentano modi di vibrazione del grafo e possono essere interpretati come descrizioni delle posizioni relative dei nodi.
- Gli **autovalori** indicano la frequenza di queste vibrazioni.
#### **Come funziona?**

1. Si calcola la matrice Laplaciana L del grafo.
2. Si calcolano gli autovalori e gli autovettori di L.
3. Si selezionano i primi k autovettori (escludendo l'autovettore associato all'autovalore 0, che rappresenta la componente costante).
4. Ogni nodo del grafo viene rappresentato da una combinazione degli autovettori selezionati, formando un vettore posizionale.

#### **Perché è utile?**

- Gli autovettori catturano la struttura globale del grafo, inclusi cluster, connettività e distanze tra nodi.
- Questa codifica può essere utilizzata come input aggiuntivo in modelli GNN per migliorare la capacità del modello di distinguere tra nodi simili ma non identici.

#### **Esempio di applicazione**

In un grafo sociale, gli autovettori della Laplaciana possono rivelare comunità o gruppi di nodi strettamente connessi. Queste informazioni possono essere incorporate in un modello GNN per migliorare la classificazione dei nodi o la previsione dei link.

### **Random Walk Positional Encoding**
Il **Random Walk Positional Encoding** utilizza le proprietà delle passeggiate casuali (random walks) sul grafo per codificare la posizione dei nodi. Una passeggiata casuale è un processo stocastico in cui si passa da un nodo a un nodo adiacente con una certa probabilità.

La codifica basata su random walk cattura informazioni sulla probabilità di raggiungere un nodo partendo da un altro nodo in un certo numero di passi. Questo tipo di codifica riflette la struttura locale e globale del grafo.

#### **Come funziona?**

1. Si definisce una matrice di transizione $P$, che rappresenta le probabilità di spostarsi da un nodo a un altro in un singolo passo:$$P=D^{−1}A$$dove $D$ è la matrice dei gradi e $A$ è la matrice di adiacenza.
2. Si calcolano le potenze della matrice di transizione $P^k$, che rappresentano le probabilità di raggiungere un nodo in $k$ passi.
3. Per ogni nodo, si estraggono le probabilità di raggiungere altri nodi in un certo numero di passi (ad esempio, i primi $k$ passi).
4. Queste probabilità vengono utilizzate come codifica posizionale.

#### **Perché è utile?**

- Le passeggiate casuali catturano informazioni sia locali (connessioni immediate) che globali (raggiungibilità a lungo raggio).
- Questa codifica è particolarmente efficace per grafi con strutture complesse o asimmetriche, poiché riflette la direzione e la probabilità delle connessioni.

#### **Esempio di applicazione**

In un grafo di citazioni accademiche, le passeggiate casuali possono rivelare quanto un articolo è influente rispetto ad altri. Questa informazione può essere incorporata in un modello GNN per migliorare la classificazione o il ranking dei nodi.

### **Confronto tra Laplacian Eigenvector e Random Walk Positional Encoding**

|**Caratteristica**|**Laplacian Eigenvector**|**Random Walk**|
|---|---|---|
|**Base matematica**|Autovettori della matrice Laplaciana|Probabilità delle passeggiate casuali|
|**Informazioni catturate**|Struttura globale (cluster, connettività)|Struttura locale e globale (raggiungibilità)|
|**Complessità computazionale**|Elevata (calcolo degli autovalori/autovettori)|Moderata (calcolo delle potenze della matrice)|
|**Applicazioni tipiche**|Grafi con strutture globali ben definite|Grafi con strutture complesse o asimmetriche|
#### **Quando usare quale?**

- **Laplacian Eigenvector Positional Encoding** :    
    - Ideale per grafi con una struttura globale ben definita, come reti sociali o grafi di trasporto.
    - Utile quando si vuole enfatizzare le proprietà spettrali del grafo.
    
- **Random Walk Positional Encoding** :    
    - Ideale per grafi con strutture locali complesse o direzionali, come grafi di citazioni o reti di comunicazione.
    - Utile quando si vuole enfatizzare la probabilità di raggiungibilità tra nodi.