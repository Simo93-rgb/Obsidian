
## 1.  Cos’è una *sequenza temporale* in un problema di classificazione

In termini generali, una sequenza temporale è un insieme ordinato di eventi \((t_i, x_i)\) dove

* **\(t_i\)** è l’istante in cui l’evento \(x_i\) è stato registrato (data, ora, durata, ecc.);
* **\(x_i\)** è l’osservazione in quel momento (in questo caso un’azione clinica codificata in un vocabolario specifico).

Il compito di *classificazione binaria* è quindi di predire un’etichetta \(y \in \{0,1\}\) per **tutte** le sequenze (o per ogni evento all’interno della sequenza) basandosi su tutti i suoi elementi \(\{(t_i, x_i)\}_{i=1}^L\).

> **Formalizzazione**  
> \[
> \text{Input: } \mathbf{s} = [(t_1,x_1),(t_2,x_2),\dots,(t_L,x_L)]\\
> \text{Output: } \hat y = f_\theta(\mathbf{s}) \in \{0,1\}
> \]

dove \(f_\theta\) è il modello (ad es. un Transformer).

---

## 2.  Come vengono normalmente *modellate* le sequenze temporali

| Approccio | Vantaggi | Svantaggi | Quando usarlo |
|-----------|----------|-----------|---------------|
| **RNN / LSTM / GRU** | Gestione di lunghezze variabili, ricordi a lungo termine | Latenza in inferenza, difficoltà con dipendenze molto lunghe | Dataset piccolo / sequenze corti |
| **CNN 1‑D** (Temporal Convolutional Network) | Parallelizzazione veloce, capacità di catturare pattern locali | Necessità di padding & dilatazioni per coprire lunghe dipendenze | Sequenze di media lunghezza |
| **Transformer** (self‑attention) | Cattura dipendenze a distanza, parallelo, scalabile | Richiede più dati, memoria proporzionale alla lunghezza | Sequenze lunghe, dati numerosi |
| **Temporal Point Process (Hawkes, Poisson)** | Modellamento esplicito delle distanze temporali | Complesso da addestrare, meno comune per classificazione | Eventi rari, tempo continuo |
| **Neural ODE / SDE** | Continua tra gli eventi, gestisce dati irregolari | Computazione intensiva | Tempo continuo, dati sporadici |
| **Hybrid (RNN+Attention)** | Bilancia ricordo a lungo termine + attenzione | Modellazione complessa | Se sequenze sono lunghe ma hanno parti “ricorrenti” |

Per **BERT‑based** modelli è più comune trattare la sequenza come *testo* e quindi usarlo come **sequence‑classification head** (ad es. `bert-base-uncased` fine‑tuned con un classificatore lineare sulla rappresentazione `[CLS]`). Ma **BERT** di per sé non è “time‑aware”: il suo encoding posizionale è relativo alla posizione dell’unità nella sequenza, non al **tempo reale**. Per i dati clinici è dunque cruciale *incorporare* le informazioni temporali.

---

## 3.  Come incorporare la *distanza temporale* nei modelli Transformer

| Metodo | Come funziona | Note |
|--------|---------------|------|
| **Sinusoidal Positional Encoding + Time‑Delta Token** | Ogni token \(x_i\) ha un embedding \(\mathbf{e}_{x_i}\); si aggiunge un embedding di *tempo* \(\mathbf{e}_{\Delta t_i}\) dove \(\Delta t_i = t_i - t_{i-1}\). | Semplice, ma richiede una tokenizzazione per gli intervalli temporali. |
| **Learned Time Embedding** | \(\Delta t_i\) viene mappato su un vettore tramite una rete densa o embedding look‑up (dovrebbe essere discretizzato o normalizzato). | Più flessibile, può catturare non‑linearitá temporali. |
| **Time2Vec** | Combina componenti sinusoidali e lineari di \(\Delta t\) in un unico embedding continuo. | Buono per interi intervalli continui. |
| **Relative Positional Encoding (Transformer‑XL / T5)** | Usa differenze di posizione per calcolare la self‑attention. Si può estendere a differenze di tempo. | Non richiede padding. |
| **Time‑Aware Attention (e.g., Time‑Transformer, Time‑BERT)** | L’attenzione è modulata da funzioni di tempo (decay, gating). | Specifico per serie temporali, è stato testato su dati medici. |
| **Continuous‑time Transformer (CTC‑Transformer)** | Interpolazione continua tra eventi, attenzione su finestre temporali. | Più complesso, meno usato. |

**Pratica consigliata**  
1. **Pre‑processing**: calcola \(\Delta t_i\) per ogni evento; normalizza (es. `min-max` o `log`) se gli intervalli sono molto variegati.  
2. **Tokenizzazione**: crea un *vocabolo* per azioni cliniche + un vocabolario per “tempo‑tokens” (es. `T_0.5d`, `T_1w`).  
3. **Embedding**: `e_total = e_action + e_time + e_positional`.  
4. **Fine‑tuning**: addestra `bert-base-uncased` con la loss di classificazione binaria (BCE) e l’attenzione mask.

---

## 4.  Problemi teorici e pratici da tenere in mente

| Tema | Cosa considerare | Perché è importante |
|------|------------------|---------------------|
| **Irregolarità e lunghezze diverse** | Padding, attention masks, batch‑wise length sorting | Il modello deve gestire lunghezze diverse senza introdurre bias |
| **Dati mancanti / sparse** | Imputazione (zero, mean, interpolazione), token `MISSING` | Evita che il modello impari a predire basandosi sul pattern di assenza |
| **Bilanciamento delle classi** | Class‑weights, focal loss, oversampling | I dati clinici spesso hanno etichette rare |
| **Feature engineering** | Intervalli di tempo, frequenza di azioni, aggregazioni (mean, max) | Aggiunge informazioni non presenti nei token |
| **Interpretabilità** | Visualizzare heat‑maps di attenzione, saliency, SHAP | Importante in ambito medico per spiegare decisioni |
| **Valutazione** | ROC‑AUC, Precision‑Recall, F1‑macro | Dipende dalla priorità (es. riduzione di falsi negativi) |
| **Over‑fitting** | Early stopping, dropout, weight decay | I dataset clinici spesso piccoli |
| **Pre‑training** | BioBERT, ClinicalBERT, PubMedBERT | Meno dati, meglio fine‑tune su dominio |

---

## 5.  Workflow consigliato

1. **Acquisizione & pulizia**  
   * Rimuovere duplicati, normalizzare date, gestire fusi orari.  
   * Rimuovere/normalizzare valori anomali.

2. **Feature‑engineering temporale**  
   * Calcolare \(\Delta t_i\), \(\log(1+\Delta t_i)\), ecc.  
   * Creare feature di frequenza di azioni (es. numero di iniezioni in una settimana).

3. **Tokenizzazione**  
   * Azioni cliniche → ID in `vocab_actions`.  
   * Intervalli → ID in `vocab_time`.  
   * Posizionamento → embeddings sinusoidali.

4. **Modellazione**  
   * `bert-base-uncased` fine‑tuned con `Linear` head.  
   * Addirittura aggiungere un “time‑token” al `[CLS]` (es. `T_0.5d`) per dare un contesto temporale all’inizio.

5. **Addestramento**  
   * Learning rate `5e-5`–`3e-5`.  
   * Batch size 8–16 (dipende dalla GPU).  
   * Early stopping sul validation loss / AUC.

6. **Valutazione**  
   * ROC‑AUC, PR‑AUC (specialmente per classi rare).  
   * Analisi degli errori: false positive vs false negative.

7. **Deploy**  
   * Pipeline di inferenza: tokenizza, normalizza tempi, passa al modello, restituisci etichetta + probabilità.  
   * Log dei risultati per audit.

---

## 6.  Approcci e modelli che meritano studio

| Modello | Applicazione | Link |
|---------|--------------|------|
| **Time‑Transformer** | Series classification con attenzione temporale | <https://arxiv.org/abs/2108.10785> |
| **Time‑BERT (Time‑Aware BERT)** | Sequenze di eventi clinici | <https://arxiv.org/abs/2006.09692> |
| **Temporal Fusion Transformer (TFT)** | Forecasting + classificazione tabulare | <https://arxiv.org/abs/1912.09363> |
| **Hawkes Process + Transformer** | Modello di punti temporali | <https://arxiv.org/abs/2004.10890> |
| **Neural ODE for time series** | Continua tra eventi | <https://arxiv.org/abs/1806.07366> |
| **T5 (Text‑to‑Text Transfer Transformer)** con time‑tokens | Se vuoi sfruttare la generatività | <https://arxiv.org/abs/1910.10683> |
| **ClinicalBERT / BioBERT** | Pre‑training su corpora medici | <https://arxiv.org/abs/1904.05342>, <https://arxiv.org/abs/1907.11692> |

> **Consiglio pratico**  
> Inizia con un semplice *time‑token* concatenato al `[CLS]` e usa `BERT‑base`. Se i risultati non sono soddisfacenti, passa a **Time‑BERT** o **Temporal Fusion Transformer**.

---

## 7.  Punti chiave da ricordare

1. **La “temporale” non è solo posizione**: il modello deve capire *quanto tempo è passato* tra gli eventi.  
2. **Encoder‑positional** di BERT è relativo; devi aggiungere un *time‑embedding* per la reale distanza.  
3. **Dataset di grandi dimensioni** è preferibile per Transformer, ma i modelli pre‑addestrati (e.g. BioBERT) riducono la dipendenza dal volume.  
4. **Valutazione rigorosa**: in ambito clinico, la **precauzione** (riduzione di falsi negativi) spesso pesa più del semplice AUC.  
5. **Interpretabilità** è fondamentale; usa l’attenzione o strumenti di explainable AI per confermare che il modello "legge" le azioni cliniche e non i soli pattern di tempo.

Con questi punti teorici e pratici sarai in grado di formalizzare la tua problematica, scegliere l’architettura più adatta e procedere in modo strutturato al fine‑tuning del tuo Transformer. Buona fortuna con il tuo progetto!