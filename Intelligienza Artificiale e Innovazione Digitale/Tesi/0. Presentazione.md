# Primo Scambio di Email

Ciao Simone,

scusa se mi sono preso un po' di tempo per risponderti. Sono contento che ti interessi il progetto riguardante l'utilizzo dei Transformer per la classificazione delle tracce sanitarie.

Ho già iniziato a riprendere il lavoro sul Transformer, studiando come funziona e quali sono le sue prestazioni con il nuovo dataset. L'idea è di applicare questa tecnologia alle nuove tracce provenienti dall'Ospedale di Alessandria per identificare eventuali ritardi nelle procedure mediche.

L'architettura che stiamo usando è una tipica encoder-decoder, progettata per generare token e ricostruire le azioni mancanti basandosi su un prefisso di traccia. L'implementazione originale è stata sviluppata da un dottorando ora a Oxford e ha già mostrato risultati promettenti con pazienti affetti da ictus.

Per il nuovo contesto, vorremmo classificare le tracce come "normali" o "problematiche", basandoci su potenziali colli di bottiglia che potrebbero allungare la permanenza dei pazienti in ospedale. Inoltre, un altro aspetto importante del progetto sarà quello dell'explicabilità (explainability), sfruttando le attention maps per capire quali azioni influenzano maggiormente l'assegnazione della classe.

Ti metterei volentieri a lavorare in questo contesto se sei interessato. Ci saranno molte possibilità di crescita e apprendimento.

# Progetto

## Obiettivo
Utilizzo di un Transformer per classificare tracce di processo in ambito sanitario.

## Descrizione
L'architettura utilizzata è una tipica encoder-decoder. Il modello viene addestrato con sequenze di azioni, ognuna dotata di timestamp, affinché il Transformer possa generare token e ricostruire automaticamente le azioni rimanenti dato un prefisso di traccia.

## Approccio
Aggiungere un'azione "fittizia" alla fine delle tracce per rappresentare la classe. Il modello genera i token mancanti, e l'ultimo token generato dovrebbe rappresentare la classe assegnata.

## Applicazione
Test sulle tracce di pazienti affetti da ictus (stroke) è stato efficace; ora si applicherà su nuove tracce dall'Ospedale di Alessandria per classificarle in "normali" o "problematiche".

## Focalizzazione
Analisi delle potenziali ritardi nell'esecuzione delle procedure mediche.

## Competenze Necessarie

### 1. Conoscenza dei Transformers

#### Fondamenti Teorici:

- **Architettura del Transformer**: Leggi la paper originale "Attention is All You Need" di Vaswani et al. per capire l'architettura base, comprese le componenti Encoder e Decoder.
- **Self-attention meccanismo**: Studia come funziona il meccanismo di attenzione multipla e perché è centrale nei Transformers.

### 2. Elaborazione del Linguaggio Naturale (NLP) e Process Mining

#### NLP:

- **Basics of Text Preprocessing**: Leggi sulla tokenizzazione, lemmatizzazione, e stemming.
- **Sequence Modeling**: Studia l'uso dei modelli RNN, LSTM, GRU nel contesto di NLP e come sono stati superati dai Transformer.

#### Process Mining:

- **Concetti Fondamentali**: Familiarizza con termini chiave come event logs, process discovery, e conformance checking.
- **Applicazioni del Process Mining in Sanità**: Analizza come i dati delle tracce sanitarie possono essere utilizzati per scoprire modelli di processo.

### 6. Interpretazione e Spiegabilità (Explainability)

#### Teoria dell’Esplicabilità:

- **Attention Maps**: Approfondisci come le attenzioni nei Transformer possono essere utilizzate per spiegare le decisioni del modello.
- **Metodi di Spiegazione Modelli Complessi**: Studia strumenti e tecniche come LIME, SHAP per interpretare modelli complessi.
