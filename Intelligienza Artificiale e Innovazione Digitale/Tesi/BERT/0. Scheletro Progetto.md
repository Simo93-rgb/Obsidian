**Obiettivo Primario**: Sviluppare un classificatore binario basato su BERT per predire la classe del paziente (`class_0` o `class_1`) basandosi sulla sequenza di attivitÃ  cliniche. Estrarre e analizzare le mappe di attenzione per comprendere quali attivitÃ  (token) influenzano maggiormente la decisione del modello.

**Lingua e Modello**:

1. **Traduzione**: Per semplificare la scelta del modello BERT e sfruttare i numerosi modelli pre-addestrati disponibili, il dataset verrÃ  **tradotto in inglese** come primo passo.
2. **Tokenizzazione**: Useremo il tokenizzatore standard di un modello BERT pre-addestrato in inglese (es. `bert-base-uncased` o, se le attivitÃ  tradotte mantengono una terminologia clinica specifica, potremmo considerare `dmis-lab/biobert-base-cased-v1.2` o simili, verificandone la compatibilitÃ  con l'inglese generale delle attivitÃ ). **Non useremo il `vocab.txt` fornito direttamente** per la tokenizzazione BERT, ma le stringhe di attivitÃ  (tradotte) verranno processate dal tokenizzatore del modello scelto.
3. **Modello Base**: `BertForSequenceClassification` di Hugging Face, configurato per la classificazione binaria.

## ğŸ§¬ Strategia di Preprocessing e Input per BERT
- **Raggruppamento e Ordinamento**:
    
    - I dati verranno raggruppati per `case_id`.
    - Per ogni `case_id`, le attivitÃ  verranno ordinate cronologicamente usando il `timestamp`.
- **Gestione Etichetta**:
    
    - L'ultima attivitÃ  (che rappresenta `class_0` o `class_1`) verrÃ  separata e usata come etichetta target. Non farÃ  parte della sequenza di input data a BERT per la predizione.
- **Integrazione Informazioni Temporali (Delta Temporali)**:
    
    - Calcoleremo la differenza di tempo (delta) tra attivitÃ  consecutive.
    - Questi delta verranno **discretizzati in categorie testuali** (es. `_TIME_DELTA_SECONDS_0-60_`, `_TIME_DELTA_MINUTES_1-60_`, `_TIME_DELTA_HOURS_1-24_`, `_TIME_DELTA_DAYS_GT1_`). Questi "token temporali" verranno inseriti nella sequenza tra le attivitÃ .
        - _Esempio_: `activity_A _TIME_DELTA_MINUTES_1-60_ activity_B`
    - Questi nuovi token testuali verranno trattati come parole normali dal tokenizzatore di BERT.
- **Costruzione Sequenza di Input**:
    
    - Per ogni paziente, la sequenza di input per BERT sarÃ  una stringa concatenata: `"[CLS] translated_activity_1 <time_delta_1_token> translated_activity_2 <time_delta_2_token> ... translated_activity_N [SEP]"`
    - **NON** useremo `<sos>` o `<eos>` personalizzati, poichÃ© `[CLS]` e `[SEP]` sono i token speciali standard di BERT che svolgono funzioni analoghe per l'inizio della sequenza e la separazione/fine.
- **Gestione Lunghezza Sequenze**:
    
    - Data la lunghezza massima elevata (fino a 8228 token), e considerando che BERT base ha un limite tipico di 512 token, inizieremo con una strategia di **troncamento** delle sequenze piÃ¹ lunghe (es. prendendo gli ultimi `k` token o i primi `k` token, da decidere in base a dove si presume sia l'informazione piÃ¹ rilevante).
    - Questa Ã¨ una semplificazione; in futuro si potrebbero esplorare modelli per sequenze lunghe (Longformer, BigBird) o approcci gerarchici.
- **Gestione Sbilanciamento Classi (7:1)**:
    
    - Durante l'addestramento, applicheremo una **weighted loss function** (es. `CrossEntropyLoss` con pesi calcolati in base all'inverso della frequenza delle classi) per dare piÃ¹ importanza alla classe minoritaria.
    - Le metriche di valutazione terranno conto dello sbilanciamento (F1-score, Precision, Recall, AUC-ROC, Confusion Matrix).

## ğŸ—ï¸ Struttura Proposta delle Cartelle e File Sorgente
```plaintext
bert_patient_classification/
â”‚ 
â”œâ”€â”€ data/ # Dati grezzi e processati 
â”‚   â”œâ”€â”€ your_dataset.csv # Il tuo CSV originale 
â”‚   â””â”€â”€ processed/ # Dati pronti per il training 
â”‚       â”œâ”€â”€ train_english.csv # CSV tradotto e pre-processato 
â”‚       â”œâ”€â”€ val_english.csv 
â”‚       â””â”€â”€ test_english.csv 
â”‚ 
â”œâ”€â”€ notebooks/ # Jupyter notebooks per analisi esplorativa e visualizzazioni 
â”‚   â”œâ”€â”€ 00_data_translation.ipynb # (Opzionale) Notebook per tradurre e salvare il dataset 
â”‚   â”œâ”€â”€ 01_data_exploration_and_preprocessing.ipynb # Analisi, calcolo delta, creazione sequenze 
â”‚   â”œâ”€â”€ 02_tokenization_and_dataset_creation.ipynb # Sperimentazione tokenizzazione, creazione PyTorch Datasets 
â”‚   â”œâ”€â”€ 03_model_training_evaluation.ipynb # Training loop, valutazione 
â”‚   â”œâ”€â”€ 04_attention_analysis.ipynb # Estrazione e visualizzazione mappe di attenzione 
â”‚   
â”œâ”€â”€ src/ # Codice sorgente modulare 
â”‚   â”œâ”€â”€ __init__.py 
â”‚   â”œâ”€â”€ config.py # Variabili di configurazione (path, nomi modelli, iperparametri) 
â”‚   â”œâ”€â”€ data_loader.py # Classe PyTorch Dataset, DataLoaders, funzioni di preprocessing 
â”‚   â”œâ”€â”€ model.py # Definizione del modello BERT (BertForSequenceClassification), funzioni per attenzioni 
â”‚   â”œâ”€â”€ train.py # Script principale per l'addestramento del modello â”‚   â”œâ”€â”€ evaluate.py # Script per la valutazione del modello su un test set 
â”‚   â””â”€â”€ utils.py # Funzioni di utilitÃ  (es. calcolo pesi per loss, salvataggio/caricamento, ecc.) 
â”‚ 
â”œâ”€â”€ results/ # Output del progetto 
â”‚   â”œâ”€â”€ models/ # Checkpoint dei modelli addestrati (.bin, config.json) 
â”‚   â”œâ”€â”€ logs/ # Log di training (es. TensorBoard logs o file di testo) 
â”‚   â”œâ”€â”€ plots/ # Grafici (curve di learning, matrici di confusione, visualizzazioni attenzioni) 
â”‚   â””â”€â”€ reports/ # Report di valutazione, tabelle di metriche 
â”‚ 
â”œâ”€â”€ requirements.txt # Dipendenze Python (elenco librerie e versioni) 
â”œâ”€â”€ README.md # Descrizione del progetto, istruzioni setup ed esecuzione 
â””â”€â”€ .env.example # (Opzionale) Esempio per variabili d'ambiente
```

## ğŸ“ Fasi del Progetto e Implementazione

- **Setup Ambiente**:
    - Creare ambiente virtuale Python.
    - Installare `transformers`, `torch`, `pandas`, `scikit-learn`, `matplotlib`, `seaborn`, e una libreria per la traduzione (es. `googletrans` o simili, oppure pre-tradurre manualmente/con altri tool).
- **Fase 1: Preprocessing Dati (`data_loader.py`, `notebooks/00_` e `01_`)**
    - **Traduzione**: Tradurre la colonna `activity` in inglese.
    - **Caricamento e Pulizia**: Caricare il CSV.
    - **Creazione Sequenze**:
        - Raggruppare per `case_id`.
        - Ordinare per `timestamp`.
        - Estrarre l'etichetta di classe (ultima attivitÃ ).
        - Calcolare i delta temporali tra attivitÃ .
        - Discretizzare i delta in token testuali.
        - Assemblare le sequenze di input (stringhe).
    - **Split Dataset**: Suddividere in set di training, validazione e test, mantenendo l'integritÃ  dei `case_id` (un paziente deve stare interamente in un solo set).
- **Fase 2: Tokenizzazione e Creazione `Dataset` PyTorch (`data_loader.py`, `notebooks/02_`)**
    - Inizializzare il tokenizzatore BERT scelto.
    - Tokenizzare le sequenze di input (con i token temporali testuali).
    - Gestire padding e troncamento per creare batch di lunghezza uniforme.
    - Creare una classe `torch.utils.data.Dataset` personalizzata.
    - Creare `DataLoaders` per training, validazione e test.
- **Fase 3: Definizione e Addestramento Modello (`model.py`, `train.py`, `notebooks/03_`)**
    - Caricare `BertForSequenceClassification` pre-addestrato, specificando `num_labels=2` e `output_attentions=True`.
    - Definire l'ottimizzatore (es. AdamW) e lo scheduler del learning rate.
    - Implementare il training loop:
        - Forward pass.
        - Calcolo della loss pesata.
        - Backward pass e aggiornamento pesi.
    - Loop di validazione per monitorare le performance e fare early stopping.
    - Salvare i checkpoint del modello migliore.
- **Fase 4: Valutazione (`evaluate.py`, `notebooks/03_`)**
    - Caricare il modello migliore.
    - Valutare sul test set.
    - Calcolare metriche: accuratezza, precisione, recall, F1-score (macro/micro/weighted), ROC-AUC, matrice di confusione.
- **Fase 5: Estrazione e Analisi Mappe di Attenzione (`model.py`, `utils.py`, `notebooks/04_`)**
    - Scrivere funzioni per passare un input al modello e recuperare gli `attentions` dall'output.
    - Analizzare le attenzioni:
        - Mediare gli head di attenzione per layer?
        - Concentrarsi sull'ultimo layer o su tutti?
        - Come i token di input (attivitÃ  e token temporali) contribuiscono alla rappresentazione del token `[CLS]` (usato per la classificazione).
    - Visualizzare le attenzioni (es. heatmap che mostrano il peso dell'attenzione da ogni token di input al token `[CLS]` o tra i token di input stessi).