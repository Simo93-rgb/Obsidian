**Obiettivo Primario**: Sviluppare un classificatore binario basato su BERT per predire la classe del paziente (`class_0` o `class_1`) basandosi sulla sequenza di attività cliniche. Estrarre e analizzare le mappe di attenzione per comprendere quali attività (token) influenzano maggiormente la decisione del modello.

**Lingua e Modello**:

1. **Traduzione**: Per semplificare la scelta del modello BERT e sfruttare i numerosi modelli pre-addestrati disponibili, il dataset verrà **tradotto in inglese** come primo passo.
2. **Tokenizzazione**: Useremo il tokenizzatore standard di un modello BERT pre-addestrato in inglese (es. `bert-base-uncased` o, se le attività tradotte mantengono una terminologia clinica specifica, potremmo considerare `dmis-lab/biobert-base-cased-v1.2` o simili, verificandone la compatibilità con l'inglese generale delle attività). **Non useremo il `vocab.txt` fornito direttamente** per la tokenizzazione BERT, ma le stringhe di attività (tradotte) verranno processate dal tokenizzatore del modello scelto.
3. **Modello Base**: `BertForSequenceClassification` di Hugging Face, configurato per la classificazione binaria.

## 🧬 Strategia di Preprocessing e Input per BERT
- **Raggruppamento e Ordinamento**:
    
    - I dati verranno raggruppati per `case_id`.
    - Per ogni `case_id`, le attività verranno ordinate cronologicamente usando il `timestamp`.
- **Gestione Etichetta**:
    
    - L'ultima attività (che rappresenta `class_0` o `class_1`) verrà separata e usata come etichetta target. Non farà parte della sequenza di input data a BERT per la predizione.
- **Integrazione Informazioni Temporali (Delta Temporali)**:
    
    - Calcoleremo la differenza di tempo (delta) tra attività consecutive.
    - Questi delta verranno **discretizzati in categorie testuali** (es. `_TIME_DELTA_SECONDS_0-60_`, `_TIME_DELTA_MINUTES_1-60_`, `_TIME_DELTA_HOURS_1-24_`, `_TIME_DELTA_DAYS_GT1_`). Questi "token temporali" verranno inseriti nella sequenza tra le attività.
        - _Esempio_: `activity_A _TIME_DELTA_MINUTES_1-60_ activity_B`
    - Questi nuovi token testuali verranno trattati come parole normali dal tokenizzatore di BERT.
- **Costruzione Sequenza di Input**:
    
    - Per ogni paziente, la sequenza di input per BERT sarà una stringa concatenata: `"[CLS] translated_activity_1 <time_delta_1_token> translated_activity_2 <time_delta_2_token> ... translated_activity_N [SEP]"`
    - **NON** useremo `<sos>` o `<eos>` personalizzati, poiché `[CLS]` e `[SEP]` sono i token speciali standard di BERT che svolgono funzioni analoghe per l'inizio della sequenza e la separazione/fine.
- **Gestione Lunghezza Sequenze**:
    
    - Data la lunghezza massima elevata (fino a 8228 token), e considerando che BERT base ha un limite tipico di 512 token, inizieremo con una strategia di **troncamento** delle sequenze più lunghe (es. prendendo gli ultimi `k` token o i primi `k` token, da decidere in base a dove si presume sia l'informazione più rilevante).
    - Questa è una semplificazione; in futuro si potrebbero esplorare modelli per sequenze lunghe (Longformer, BigBird) o approcci gerarchici.
- **Gestione Sbilanciamento Classi (7:1)**:
    
    - Durante l'addestramento, applicheremo una **weighted loss function** (es. `CrossEntropyLoss` con pesi calcolati in base all'inverso della frequenza delle classi) per dare più importanza alla classe minoritaria.
    - Le metriche di valutazione terranno conto dello sbilanciamento (F1-score, Precision, Recall, AUC-ROC, Confusion Matrix).

## 🏗️ Struttura Proposta delle Cartelle e File Sorgente
```plaintext
bert_patient_classification/
│ 
├── data/ # Dati grezzi e processati 
│   ├── your_dataset.csv # Il tuo CSV originale 
│   └── processed/ # Dati pronti per il training 
│       ├── train_english.csv # CSV tradotto e pre-processato 
│       ├── val_english.csv 
│       └── test_english.csv 
│ 
├── notebooks/ # Jupyter notebooks per analisi esplorativa e visualizzazioni 
│   ├── 00_data_translation.ipynb # (Opzionale) Notebook per tradurre e salvare il dataset 
│   ├── 01_data_exploration_and_preprocessing.ipynb # Analisi, calcolo delta, creazione sequenze 
│   ├── 02_tokenization_and_dataset_creation.ipynb # Sperimentazione tokenizzazione, creazione PyTorch Datasets 
│   ├── 03_model_training_evaluation.ipynb # Training loop, valutazione 
│   ├── 04_attention_analysis.ipynb # Estrazione e visualizzazione mappe di attenzione 
│   
├── src/ # Codice sorgente modulare 
│   ├── __init__.py 
│   ├── config.py # Variabili di configurazione (path, nomi modelli, iperparametri) 
│   ├── data_loader.py # Classe PyTorch Dataset, DataLoaders, funzioni di preprocessing 
│   ├── model.py # Definizione del modello BERT (BertForSequenceClassification), funzioni per attenzioni 
│   ├── train.py # Script principale per l'addestramento del modello │   ├── evaluate.py # Script per la valutazione del modello su un test set 
│   └── utils.py # Funzioni di utilità (es. calcolo pesi per loss, salvataggio/caricamento, ecc.) 
│ 
├── results/ # Output del progetto 
│   ├── models/ # Checkpoint dei modelli addestrati (.bin, config.json) 
│   ├── logs/ # Log di training (es. TensorBoard logs o file di testo) 
│   ├── plots/ # Grafici (curve di learning, matrici di confusione, visualizzazioni attenzioni) 
│   └── reports/ # Report di valutazione, tabelle di metriche 
│ 
├── requirements.txt # Dipendenze Python (elenco librerie e versioni) 
├── README.md # Descrizione del progetto, istruzioni setup ed esecuzione 
└── .env.example # (Opzionale) Esempio per variabili d'ambiente
```

## 📝 Fasi del Progetto e Implementazione

- **Setup Ambiente**:
    - Creare ambiente virtuale Python.
    - Installare `transformers`, `torch`, `pandas`, `scikit-learn`, `matplotlib`, `seaborn`, e una libreria per la traduzione (es. `googletrans` o simili, oppure pre-tradurre manualmente/con altri tool).
- **Fase 1: Preprocessing Dati (`data_loader.py`, `notebooks/00_` e `01_`)**
    - **Traduzione**: Tradurre la colonna `activity` in inglese.
    - **Caricamento e Pulizia**: Caricare il CSV.
    - **Creazione Sequenze**:
        - Raggruppare per `case_id`.
        - Ordinare per `timestamp`.
        - Estrarre l'etichetta di classe (ultima attività).
        - Calcolare i delta temporali tra attività.
        - Discretizzare i delta in token testuali.
        - Assemblare le sequenze di input (stringhe).
    - **Split Dataset**: Suddividere in set di training, validazione e test, mantenendo l'integrità dei `case_id` (un paziente deve stare interamente in un solo set).
- **Fase 2: Tokenizzazione e Creazione `Dataset` PyTorch (`data_loader.py`, `notebooks/02_`)**
    - Inizializzare il tokenizzatore BERT scelto.
    - Tokenizzare le sequenze di input (con i token temporali testuali).
    - Gestire padding e troncamento per creare batch di lunghezza uniforme.
    - Creare una classe `torch.utils.data.Dataset` personalizzata.
    - Creare `DataLoaders` per training, validazione e test.
- **Fase 3: Definizione e Addestramento Modello (`model.py`, `train.py`, `notebooks/03_`)**
    - Caricare `BertForSequenceClassification` pre-addestrato, specificando `num_labels=2` e `output_attentions=True`.
    - Definire l'ottimizzatore (es. AdamW) e lo scheduler del learning rate.
    - Implementare il training loop:
        - Forward pass.
        - Calcolo della loss pesata.
        - Backward pass e aggiornamento pesi.
    - Loop di validazione per monitorare le performance e fare early stopping.
    - Salvare i checkpoint del modello migliore.
- **Fase 4: Valutazione (`evaluate.py`, `notebooks/03_`)**
    - Caricare il modello migliore.
    - Valutare sul test set.
    - Calcolare metriche: accuratezza, precisione, recall, F1-score (macro/micro/weighted), ROC-AUC, matrice di confusione.
- **Fase 5: Estrazione e Analisi Mappe di Attenzione (`model.py`, `utils.py`, `notebooks/04_`)**
    - Scrivere funzioni per passare un input al modello e recuperare gli `attentions` dall'output.
    - Analizzare le attenzioni:
        - Mediare gli head di attenzione per layer?
        - Concentrarsi sull'ultimo layer o su tutti?
        - Come i token di input (attività e token temporali) contribuiscono alla rappresentazione del token `[CLS]` (usato per la classificazione).
    - Visualizzare le attenzioni (es. heatmap che mostrano il peso dell'attenzione da ogni token di input al token `[CLS]` o tra i token di input stessi).