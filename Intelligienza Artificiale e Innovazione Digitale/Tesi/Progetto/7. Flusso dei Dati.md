Descrizione del flusso dei dati nel Proformer
Il flusso dei dati nel progetto Proformer segue un percorso ben definito attraverso le diverse componenti del sistema. Ecco una descrizione dettagliata:
### 1. Caricamento iniziale dei dati
Il processo inizia nella funzione _initialize_dataloader che crea un'istanza di Dataloader
Il Dataloader carica i dati da un file CSV e li prepara per l'elaborazione
Il metodo get_dataset suddivide i dati in set di training, validation e test
### 2. Preparazione delle sequenze
Ogni sequenza viene processata con il metodo process_seq che:
Aggiunge token speciali (``<sos>``, ``<eos>``)
Applica mascheramento (``<mask>``) su token specifici (per implementare un approccio simile a BERT)
Aggiunge padding se necessario per raggiungere lunghezza fissa.
### 3. Creazione dei batch
Le sequenze processate vengono organizzate in batch usando batchify_sequences
I batch sono tensori di dimensione (batch_size, sequence_length)
Le etichette di classificazione vengono mantenute separate nei tensori train_labels, valid_labels e test_labels
### 4. Durante l'addestramento
Per ogni batch di training:
Si ottiene il batch con get_batch_from_list(loader.train_data, batch_idx)
Si crea una maschera di attenzione con create_attention_mask
Il modello elabora i dati e produce logits per la predizione dei token
Per la classificazione binaria, si estraggono le etichette con get_labels_for_batch_type("train", batch_idx)
Si calcolano due loss: seq_loss per la predizione dei token e cls_loss per la classificazione
### 5. Durante la valutazione
Processo simile al training, ma:
Si usano i batch da valid_data o test_data
Ãˆ fondamentale passare il parametro data_type corretto a evaluate ("valid" o "test")
Per la classificazione, si usano solo le posizioni mascherate con get_masked_batch_labels(data_type, batch_idx, mask_positions)
Si calcolano metriche di accuratezza e classificazione
### 6. Flusso nel modello Transformer
I token in input vengono convertiti in embedding
Gli embedding passano attraverso il transformer encoder
Gli hidden states alle posizioni mascherate vengono usati per la classificazione binaria
Gli output completi vengono usati per la predizione dei token successivi
Questo approccio dual-task permette al modello di apprendere sia la struttura sequenziale del processo che la classificazione delle sequenze, ottimizzando entrambi gli obiettivi contemporaneamente.

