È stata introdotta da Vaswani et al. nel 2017 con il paper "Attention is All You Need".
[Spiegazione facile ma dettagliata](https://www.youtube.com/watch?v=zxQyTK8quyY) su YouTube gentilmente trovata da Rosilde.

## Componenti Principali di un Transformer

### Encoder-Decoder Structure

- I Transformers sono composti da due blocchi fondamentali: Encoder e Decoder, che possono essere utilizzati separatamente o insieme.
#### Encoder

- L'encoder è una serie di livelli identici. Ogni livello ha due sotto-componenti principali:
    
    - [[1.1 Multi-head Self-Attention Mechanism|Multi-head Self-Attention Mechanism]]: Questa componente calcola l'importanza delle parole in una sequenza rispetto ad altre parole nella stessa sequenza, permettendo al modello di "guardare" diverse parti della sequenza contemporaneamente. L'attenzione è multipla perché considera più "aspetti" o pattern simultaneamente.
    - **Position-wise Feed-forward Neural Network**: Una rete neurale che opera indipendentemente su ogni posizione nella sequenza, con una funzione di attivazione non lineare (come ReLU).
- **Layer Normalization e Residual Connections**: Ogni componente dell'encoder è seguita da un'operazione di normalizzazione delle caratteristiche e un salto residuale per migliorare l'addestramento della rete.
    

#### Decoder

- Il decoder ha una struttura simile a quella dell’encoder, ma con alcuni aggiustamenti:
    - Oltre alle componenti che il decoder ha in comune con l’encoder (Multi-head Self-Attention e Feed-forward Neural Network), ci sono due nuove componenti chiave:
        - **Masked Multi-head Self-Attention**: Questa componente è simile alla self-attention dell'encoder, ma include una maschera per prevenire che i dati futuri vengano utilizzati durante la previsione.
        - **Encoder-Decoder Attention**: Questo meccanismo consente al decoder di prendere in considerazione le uscite dell'encoder durante il processo decisionale.

#### Positional Encoding

- Poiché i Transformers non hanno un meccanismo intrinseco per gestire l’ordine delle sequenze (a differenza delle RNN), si utilizza un encoding posizionale aggiunto alle embedding delle parole per fornire informazioni di ordine temporale.